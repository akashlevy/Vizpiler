["11 Yacc Theory Grammars for yacc are described using a variant of Backus Naur Form (BNF). This technique, pioneered by John Backus and Peter Naur, was used to describe ALGOL60. A BNF grammar can be used to express context-free languages. Most constructs in modern programming languages can be represented in BNF. For example, the grammar for an expression that multiplies and adds numbers is E -&gt; E + E E -&gt; E * E E -&gt; id Three productions have been specified. Terms that appear on the left-hand side (lhs) of a production, such as E (expression) are nonterminals. Terms such as id (identifier) are terminals (tokens returned by lex) and only appear on the right-hand side (rhs) of a production. This grammar specifies that an expression may be the sum of two expressions, the product of two expressions, or an identifier. We can use this grammar to generate expressions: E -&gt; E * E   (r2) -&gt; E * z   (r3) -&gt; E + E * z (r1) -&gt; E + y * z (r3) -&gt; x + y * z (r3) At each step we expanded a term and replace the lhs of a production with the corresponding rhs. The numbers on the right indicate which rule applied. To parse an expression we need to do the reverse operation. Instead of starting with a single nonterminal (start symbol) and generating an expression from a grammar we need to reduce an expression to a single nonterminal. This is known as bottom-up or shift-reduce parsing and uses a stack for storing terms. Here is the same derivation but in reverse order: 1  . x + y * z  shift 2  x . + y * z  reduce(r3) 3  E . + y * z  shift 4  E + . y * z  shift 5  E + y . * z  reduce(r3) 6  E + E . * z  shift 7  E + E * . z  shift 8  E + E * z .  reduce(r3) 9  E + E * E .  reduce(r2)   emit multiply 10  E + E .   reduce(r1)   emit add 11  E .    accept Terms to the left of the dot are on the stack while remaining input is to the right of the dot. We start by shifting tokens onto the stack. When the top of the stack matches the rhs of a production we replace the matched tokens on the stack with the lhs of the production. In other words the matched tokens of the rhs are popped off the stack, and the lhs of the production is pushed on the stack. The matched tokens are known as a handle and we are reducing the handle to the lhs of the production. This process continues until we have shifted all input to the stack and only the starting nonterminal remains on the stack. In step 1 we shift the x to the stack. Step 2 applies rule r3 to the stack to change x to E . We continue shifting and reducing until a single nonterminal, the start symbol, remains in the stack. In step 9, when we reduce rule r2, we emit the multiply instruction. ","12 Similarly the add instruction is emitted in step 10. Consequently multiply has a higher precedence than addition. Consider the shift at step 6. Instead of shifting we could have reduced and apply rule r1. This would result in addition having a higher precedence than multiplication. This is known as a shift-reduce conflict. Our grammar is ambiguous because there is more than one possible derivation that will yield the expression. In this case operator precedence is affected. As another example, associativity in the rule E -&gt; E + E is ambiguous, for we may recurse on the left or the right. To remedy the situation, we could rewrite the grammar or supply yacc with directives that indicate which operator has precedence. The latter method is simpler and will be demonstrated in the practice section. The following grammar has a reduce-reduce conflict. With an id on the stack we may reduce to T , or E . E -&gt; T E -&gt; id T -&gt; id Yacc takes a default action when there is a conflict. For shift-reduce conflicts yacc will shift. For reduce-reduce conflicts it will use the first rule in the listing. It also issues a warning message whenever a conflict exists. The warnings may be suppressed by making the grammar unambiguous. Several methods for removing ambiguity will be presented in subsequent sections. Practice, Part I ... definitions ... %% ... rules ... %% ... subroutines ... Input to yacc is divided into three sections. The definitions section consists of token declarations and C code bracketed by “ %{ “ and “ %} ”. The BNF grammar is placed in the rules section and user subroutines are added in the subroutines section. This is best illustrated by constructing a small calculator that can add and subtract numbers. We’ll begin by examining the linkage between lex and yacc. Here is the definitions section for the yacc input file: %token INTEGER This definition declares an INTEGER token. Yacc generates a parser in file y.tab.c and an include file, y.tab.h : #ifndef YYSTYPE #define YYSTYPE int #endif #define INTEGER 258 extern YYSTYPE yylval; ","13 Lex includes this file and utilizes the definitions for token values. To obtain tokens yacc calls yylex . Function yylex has a return type of int that returns a token. Values associated with the token are returned by lex in variable yylval . For example, [0-9]+ { yylval = atoi(yytext); return INTEGER; } would store the value of the integer in yylval , and return token INTEGER to yacc. The type of yylval is determined by YYSTYPE . Since the default type is integer this works well in this case. Token values 0-255 are reserved for character values. For example, if you had a rule such as [-+] return *yytext;   /* return operator */ the character value for minus or plus is returned. Note that we placed the minus sign first so that it wouldn’t be mistaken for a range designator. Generated token values typically start around 258 because lex reserves several values for end-of-file and error processing. Here is the complete lex input specification for our calculator: %{ #include &lt;stdlib.h&gt; void yyerror(char *); #include y.tab.h %} %% [0-9]+ { yylval = atoi(yytext); return INTEGER; } [-+n] return *yytext; [ t] ; /* skip whitespace */ . yyerror(invalid character); %% int yywrap(void) { return 1; } Internally yacc maintains two stacks in memory; a parse stack and a value stack. The parse stack contains terminals and nonterminals that represent the current parsing state. The value stack is an array of YYSTYPE elements and associates a value with each element in the parse stack. For example when lex returns an INTEGER token yacc shifts this token to the parse stack. At the same time the corresponding yylval is shifted to the value stack. The parse and value stacks are always synchronized so finding a value related to a token on the stack is easily accomplished. Here is the yacc input specification for our calculator: ","14 %{ #include &lt;stdio.h&gt; int yylex(void); void yyerror(char *); %} %token INTEGER %% program: program expr 'n' { printf(%dn, $2); } | ; expr: INTEGER { $$ = $1; } | expr '+' expr { $$ = $1 + $3; } | expr '-' expr { $$ = $1 - $3; } ; %% void yyerror(char *s) { fprintf(stderr, %sn, s); } int main(void) { yyparse(); return 0; } The rules section resembles the BNF grammar discussed earlier. The left-hand side of a production, or nonterminal, is entered left-justified and followed by a colon. This is followed by the right-hand side of the production. Actions associated with a rule are entered in braces. With left-recursion, we have specified that a program consists of zero or more expressions. Each expression terminates with a newline. When a newline is detected we print the value of the expression. When we apply the rule expr: expr '+' expr { $$ = $1 + $3; } we replace the right-hand side of the production in the parse stack with the left-hand side of the same production. In this case we pop “ expr '+' expr ” and push “ expr ”. We have reduced the stack by popping three terms off the stack and pushing back one term. We may reference positions in the value stack in our C code by specifying “ $1 ” for the first term on the right-hand side of the production, “ $2 ” for the second, and so on. “ $$ ” designates the top of the stack after reduction has taken place. The above action adds the value associated with two expressions, pops three terms off the value stack, and pushes back a single sum. As a consequence the parse and value stacks remain synchronized. ","15 Numeric values are initially entered on the stack when we reduce from INTEGER to expr . After INTEGER is shifted to the stack we apply the rule expr: INTEGER   { $$ = $1; } The INTEGER token is popped off the parse stack followed by a push of expr . For the value stack we pop the integer value off the stack and then push it back on again. In other words we do nothing. In fact this is the default action and need not be specified. Finally, when a newline is encountered, the value associated with expr is printed. In the event of syntax errors yacc calls the user-supplied function yyerror . If you need to modify the interface to yyerror then alter the canned file that yacc includes to fit your needs. The last function in our yacc specification is main … in case you were wondering where it was. This example still has an ambiguous grammar. Although yacc will issue shift-reduce warnings it will still process the grammar using shift as the default operation. Practice, Part II In this section we will extend the calculator from the previous section to incorporate some new functionality. New features include arithmetic operators multiply and divide. Parentheses may be used to over-ride operator precedence, and single-character variables may be specified in assignment statements. The following illustrates sample input and calculator output: user: 3 * (4 + 5) calc: 27 user: x = 3 * (4 + 5) user: y = 5 user: x calc: 27 user: y calc: 5 user: x + 2*y calc: 37 ","16 The lexical analyzer returns VARIABLE and INTEGER tokens. For variables yylval specifies an index to the symbol table sym . For this program sym merely holds the value of the associated variable. When INTEGER tokens are returned, yylval contains the number scanned. Here is the input specification for lex: %{ #include &lt;stdlib.h&gt; void yyerror(char *); #include y.tab.h %} %% /* variables */ [a-z] { yylval = *yytext - 'a'; return VARIABLE; } /* integers */ [0-9]+ { yylval = atoi(yytext); return INTEGER; } /* operators */ [-+()=/*n] { return *yytext; } /* skip whitespace */ [ t] ; /* anything else is an error */ . yyerror(invalid character); %% int yywrap(void) { return 1; } ","17 The input specification for yacc follows. The tokens for INTEGER and VARIABLE are utilized by yacc to create #defines in y.tab.h for use in lex. This is followed by definitions for the arithmetic operators. We may specify %left , for left-associative or %right for right associative. The last definition listed has the highest precedence. Consequently multiplication and division have higher precedence than addition and subtraction. All four operators are left-associative. Using this simple technique we are able to disambiguate our grammar. %token INTEGER VARIABLE %left '+' '-' %left '*' '/' %{ void yyerror(char *); int yylex(void); int sym[26]; %} %% program: program statement 'n' | ; statement: expr { printf(%dn, $1); } | VARIABLE '=' expr { sym[$1] = $3; } ; expr: INTEGER | VARIABLE { $$ = sym[$1]; } | expr '+' expr { $$ = $1 + $3; } | expr '-' expr { $$ = $1 - $3; } | expr '*' expr { $$ = $1 * $3; } | expr '/' expr { $$ = $1 / $3; } | '(' expr ')' { $$ = $2; } ; %% void yyerror(char *s) { fprintf(stderr, %sn, s); return 0; } int main(void) { yyparse(); return 0; } ","18 Calculator Description This version of the calculator is substantially more complex than previous versions. Major changes include control constructs such as if-else and while . In addition a syntax tree is constructed during parsing. After parsing we walk the syntax tree to produce output. Three versions of the tree walk routine are supplied: • an interpreter that executes statements during the tree walk • a compiler that generates code for a hypothetical stack-based machine • a version that generates a syntax tree of the original program To make things more concrete, here is a sample program, x = 0; while (x &lt; 3) { print x; x = x + 1; } with output for the interpretive version, 0 1 2 an output for the compiler version, push  0 pop   x L000: push  x push  3 compLT jz   L001 push  x print push x push 1 add pop x jmp L000 L001: ","19 and a version that generates a syntax tree. Graph 0: [=] | |----| | | id(X) c(0) Graph 1: while | |----------------| | | [&lt;] [;] | | |----| |----------| | | | | id(X) c(3) print [=] | | | |-------| | | | id(X) id(X) [+] | |----| | | id(X) c(1) The include file contains declarations for the syntax tree and symbol table. Symbol table ( sym ) allows for single-character variable names. A node in the syntax tree may hold a constant ( conNodeType ), an identifier ( idNodeType ), or an internal node with an operator ( oprNodeType ). A union encapsulates all three variants and nodeType.type is used to determine which structure we have. The lex input file contains patterns for VARIABLE and INTEGER tokens. In addition, tokens are defined for 2-character operators such as EQ and NE . Single-character operators are simply returned as themselves. The yacc input file defines YYSTYPE , the type of yylval , as %union { int iValue;     /* integer value */ char sIndex;     /* symbol table index */ nodeType *nPtr;    /* node pointer */ }; This causes the following to be generated in y.tab.h: typedef union { int iValue;     /* integer value */ char sIndex;     /* symbol table index */ nodeType *nPtr;    /* node pointer */ } YYSTYPE; extern YYSTYPE yylval; ","20 Constants, variables, and nodes can be represented by yylval in the parser’s value stack. A more accurate representation of decimal integers is given below. This is similar to C/C++ where integers that begin with 0 are classified as octal. 0 { yylval.iValue = atoi(yytext); return INTEGER; } [1-9][0-9]* { yylval.iValue = atoi(yytext); return INTEGER; } Notice the type definitions %token &lt;iValue&gt; INTEGER %type &lt;nPtr&gt; expr This binds expr to nPtr , and INTEGER to iValue in the YYSTYPE union. This is required so that yacc can generate the correct code. For example, the rule expr: INTEGER { $$ = con($1); } should generate the following code. Note that yyvsp[0] addresses the top of the value stack, or the value associated with INTEGER . yylval.nPtr = con(yyvsp[0].iValue); The unary minus operator is given higher priority than binary operators as follows: %left GE LE EQ NE '&gt;' '&lt;' %left '+' '-' %left '*' '/' %nonassoc UMINUS The %nonassoc indicates no associativity is implied. It is frequently used in conjunction with %prec to specify precedence of a rule. Thus, we have expr: '-' expr %prec UMINUS { $$ = node(UMINUS, 1, $2); } indicating that the precedence of the rule is the same as the precedence of token UMINUS . And UMINUS (as defined above) has higher precedence than the other operators. A similar technique is used to remove ambiguity associated with the if-else statement (see If-Else Ambiguity). The syntax tree is constructed bottom-up allocating the leaf nodes when variables and integers are reduced. When operators are encountered a node is allocated and pointers to previously allocated nodes are entered as operands. After the tree is built function ex is called to do a depth-first walk of the syntax tree. A depth-first walk visits nodes in the order that they were originally allocated. This results in operators being applied in the order that they were encountered during parsing. Three versions of ex are included: an interpretive version, a compiler version, and a version that generates a syntax tree. ","21 Include File typedef enum { typeCon, typeId, typeOpr } nodeEnum; /* constants */ typedef struct { int value; /* value of constant */ } conNodeType; /* identifiers */ typedef struct { int i; /* subscript to sym array */ } idNodeType; /* operators */ typedef struct { int oper; /* operator */ int nops; /* number of operands */ struct nodeTypeTag *op[1]; /* operands, extended at runtime */ } oprNodeType; typedef struct nodeTypeTag { nodeEnum type; /* type of node */ union { conNodeType con; /* constants */ idNodeType id; /* identifiers */ oprNodeType opr; /* operators */ }; } nodeType; extern int sym[26]; ","23 Yacc Input %{ #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;stdarg.h&gt; #include calc3.h /* prototypes */ nodeType *opr(int oper, int nops, ...); nodeType *id(int i); nodeType *con(int value); void freeNode(nodeType *p); int ex(nodeType *p); int yylex(void); void yyerror(char *s); int sym[26]; /* symbol table */ %} %union { int iValue; /* integer value */ char sIndex; /* symbol table index */ nodeType *nPtr; /* node pointer */ }; %token &lt;iValue&gt; INTEGER %token &lt;sIndex&gt; VARIABLE %token WHILE IF PRINT %nonassoc IFX %nonassoc ELSE %left GE LE EQ NE '&gt;' '&lt;' %left '+' '-' %left '*' '/' %nonassoc UMINUS %type &lt;nPtr&gt; stmt expr stmt_list ","24 %% program: function { exit(0); } ; function: function stmt { ex($2); freeNode($2); } | /* NULL */ ; stmt: ';' { $$ = opr(';', 2, NULL, NULL); } | expr ';' { $$ = $1; } | PRINT expr ';' { $$ = opr(PRINT, 1, $2); } | VARIABLE '=' expr ';' { $$ = opr('=', 2, id($1), $3); } | WHILE '(' expr ')' stmt { $$ = opr(WHILE, 2, $3, $5); } | IF '(' expr ')' stmt %prec IFX { $$ = opr(IF, 2, $3, $5); } | IF '(' expr ')' stmt ELSE stmt { $$ = opr(IF, 3, $3, $5, $7); } | '{' stmt_list '}' { $$ = $2; } ; stmt_list: stmt { $$ = $1; } | stmt_list stmt { $$ = opr(';', 2, $1, $2); } ; expr: INTEGER { $$ = con($1); } | VARIABLE { $$ = id($1); } | '-' expr %prec UMINUS { $$ = opr(UMINUS, 1, $2); } | expr '+' expr { $$ = opr('+', 2, $1, $3); } | expr '-' expr { $$ = opr('-', 2, $1, $3); } | expr '*' expr { $$ = opr('*', 2, $1, $3); } | expr '/' expr { $$ = opr('/', 2, $1, $3); } | expr '&lt;' expr { $$ = opr('&lt;', 2, $1, $3); } | expr '&gt;' expr { $$ = opr('&gt;', 2, $1, $3); } | expr GE expr { $$ = opr(GE, 2, $1, $3); } | expr LE expr { $$ = opr(LE, 2, $1, $3); } | expr NE expr { $$ = opr(NE, 2, $1, $3); } | expr EQ expr { $$ = opr(EQ, 2, $1, $3); } | '(' expr ')' { $$ = $2; } ; ","25 %% #define SIZEOF_NODETYPE ((char *)&amp;p-&gt;con - (char *)p) nodeType *con(int value) { nodeType *p; /* allocate node */ if ((p = malloc(sizeof(nodeType))) == NULL) yyerror(out of memory); /* copy information */ p-&gt;type = typeCon; p-&gt;con.value = value; return p; } nodeType *id(int i) { nodeType *p; /* allocate node */ if ((p = malloc(sizeof(nodeType))) == NULL) yyerror(out of memory); /* copy information */ p-&gt;type = typeId; p-&gt;id.i = i; return p; } ","26 nodeType *opr(int oper, int nops, ...) { va_list ap; nodeType *p; int i; /* allocate node, extending op array */ if ((p = malloc(sizeof(nodeType) + (nops-1) * sizeof(nodeType *))) == NULL) yyerror(out of memory); /* copy information */ p-&gt;type = typeOpr; p-&gt;opr.oper = oper; p-&gt;opr.nops = nops; va_start(ap, nops); for (i = 0; i &lt; nops; i++) p-&gt;opr.op[i] = va_arg(ap, nodeType*); va_end(ap); return p; } void freeNode(nodeType *p) { int i; if (!p) return; if (p-&gt;type == typeOpr) { for (i = 0; i &lt; p-&gt;opr.nops; i++) freeNode(p-&gt;opr.op[i]); } free (p); } void yyerror(char *s) { fprintf(stdout, %sn, s); } int main(void) { yyparse(); return 0; } ","27 Interpreter #include &lt;stdio.h&gt; #include calc3.h #include y.tab.h int ex(nodeType *p) { if (!p) return 0; switch(p-&gt;type) { case typeCon: return p-&gt;con.value; case typeId: return sym[p-&gt;id.i]; case typeOpr: switch(p-&gt;opr.oper) { case WHILE: while(ex(p-&gt;opr.op[0])) ex(p-&gt;opr.op[1]); return 0; case IF: if (ex(p-&gt;opr.op[0])) ex(p-&gt;opr.op[1]); else if (p-&gt;opr.nops &gt; 2) ex(p-&gt;opr.op[2]); return 0; case PRINT: printf(%dn, ex(p-&gt;opr.op[0])); return 0; case ';': ex(p-&gt;opr.op[0]); return ex(p-&gt;opr.op[1]); case '=': return sym[p-&gt;opr.op[0]-&gt;id.i] = ex(p-&gt;opr.op[1]); case UMINUS: return -ex(p-&gt;opr.op[0]); case '+': return ex(p-&gt;opr.op[0]) + ex(p-&gt;opr.op[1]); case '-': return ex(p-&gt;opr.op[0]) - ex(p-&gt;opr.op[1]); case '*': return ex(p-&gt;opr.op[0]) * ex(p-&gt;opr.op[1]); case '/': return ex(p-&gt;opr.op[0]) / ex(p-&gt;opr.op[1]); case '&lt;': return ex(p-&gt;opr.op[0]) &lt; ex(p-&gt;opr.op[1]); case '&gt;': return ex(p-&gt;opr.op[0]) &gt; ex(p-&gt;opr.op[1]); case GE: return ex(p-&gt;opr.op[0]) &gt;= ex(p-&gt;opr.op[1]); case LE: return ex(p-&gt;opr.op[0]) &lt;= ex(p-&gt;opr.op[1]); case NE: return ex(p-&gt;opr.op[0]) != ex(p-&gt;opr.op[1]); case EQ: return ex(p-&gt;opr.op[0]) == ex(p-&gt;opr.op[1]); } } return 0; } ","36 printf(token %s (%d)n, s, yylval.ivalue); return tok; } #define RETURN(x) return dbgToken(x, #x) #define RETURN_ivalue(x) return dbgTokenIvalue(x, #x) #else #define RETURN(x) return(x) #define RETURN_ivalue(x) return(x) #endif %} %% [0-9]+ { yylval.ivalue = atoi(yytext); RETURN_ivalue(INTEGER); } if RETURN(IF); else RETURN(ELSE); More Yacc Recursion A list may be specified with left recursion list: item | list ',' item ; or right recursion. list: item | item ',' list If right recursion is used then all items on the list are pushed on the stack. After the last item is pushed we start reducing. With left recursion we never have more than three terms on the stack since we reduce as we go along. For this reason it is advantageous to use left recursion. ","37 If-Else Ambiguity A shift-reduce conflict that frequently occurs involves the if-else construct. Assume we have the following rules: stmt: IF expr stmt | IF expr stmt ELSE stmt ... and the following state: IF expr stmt IF expr stmt . ELSE stmt We need to decide if we should shift the ELSE or reduce the IF expr stmt at the top of the stack. If we shift then we have IF expr stmt IF expr stmt . ELSE stmt IF expr stmt IF expr stmt ELSE . stmt IF expr stmt IF expr stmt ELSE stmt . IF expr stmt stmt . where the second ELSE is paired with the second IF . If we reduce we have IF expr stmt IF expr stmt . ELSE stmt IF expr stmt stmt . ELSE stmt IF expr stmt . ELSE stmt IF expr stmt ELSE . stmt IF expr stmt ELSE stmt . where the second ELSE is paired with the first IF . Modern programming languages pair an ELSE with the most recent unpaired IF . Consequently the former behavior is expected. This works well with yacc because the default behavior, when a shift-reduce conflict is encountered, is to shift. Although yacc does the right thing it also issues a shift-reduce warning message. To remove the message give IF-ELSE a higher precedence than the simple IF statement: %nonassoc IFX %nonassoc ELSE stmt: IF expr stmt %prec IFX | IF expr stmt ELSE stmt ","38 Error Messages A nice compiler gives the user meaningful error messages. For example, not much information is conveyed by the following message: syntax error If we track the line number in lex then we can at least give the user a line number: void yyerror(char *s) { fprintf(stderr, line %d: %sn, yylineno, s); } When yacc discovers a parsing error the default action is to call yyerror and then return from yylex with a return value of one. A more graceful action flushes the input stream to a statement delimiter and continues to scan: stmt: ';' | expr ';' | PRINT expr ';' | VARIABLE '=' expr '; | WHILE '(' expr ')' stmt | IF '(' expr ')' stmt %prec IFX | IF '(' expr ')' stmt ELSE stmt | '{' stmt_list '}' | error ';' | error '}' ; The error token is a special feature of yacc that will match all input until the token following error is found. For this example, when yacc detects an error in a statement it will call yyerror , flush input up to the next semicolon or brace, and resume scanning. ","39 Inherited Attributes The examples so far have used synthesized attributes. At any point in a syntax tree we can determine the attributes of a node based on the attributes of its children. Consider the rule expr: expr '+' expr { $$ = $1 + $3; } Since we are parsing bottom-up, the values of both operands are available and we can determine the value associated with the left-hand side. An inherited attribute of a node depends on the value of a parent or sibling node. The following grammar defines a C variable declaration: decl: type varlist type: INT | FLOAT varlist: VAR       { setType($1, $0); } | varlist ',' VAR { setType($3, $0); } Here is a sample parse: . INT VAR INT . VAR type . VAR type VAR . type varlist . decl . When we reduce VAR to varlist we should annotate the symbol table with the type of the variable. However, the type is buried in the stack. This problem is resolved by indexing back into the stack. Recall that $1 designates the first term on the right-hand side. We can index backwards, using $0 , $-1 , and so on. In this case, $0 will do just fine. If you need to specify a token type, the syntax is $&lt;tokentype&gt;0 , angle brackets included. In this particular example care must be taken to ensure that type always precedes varlist . Embedded Actions Rules in yacc may contain embedded actions: list: item1 { do_item1($1); } item2 { do_item2($3); } item3 Note that the actions take a slot in the stack. As a result do_item2 must use $3 to reference item2 . Internally this grammar is transformed by yacc into the following: list: item1 _rule01 item2 _rule02 item3 _rule01: { do_item1($0); } _rule02: { do_item2($0); } ","40 Debugging Yacc Yacc has facilities that enable debugging. This feature may vary with different versions of yacc so be sure to consult documentation for details. The code generated by yacc in file y.tab.c includes debugging statements that are enabled by defining YYDEBUG and setting it to a non-zero value. This may also be done by specifying command-line option “ -t ”. With YYDEBUG properly set, debug output may be toggled on and off by setting yydebug . Output includes tokens scanned and shift/reduce actions. %{ #define YYDEBUG 1 %} %% ... %% int main(void) { #if YYDEBUG yydebug = 1; #endif yylex(); } In addition, you can dump the parse states by specifying command-line option  -v . States are dumped to file y.output , and are often useful when debugging a grammar. Alternatively you can write your own debug code by defining a TRACE macro as illustrated below. When DEBUG is defined a trace of reductions, by line number, is displayed. %{ #ifdef DEBUG #define TRACE printf(reduce at line %dn, __LINE__); #else #define TRACE #endif %} %% statement_list: statement { TRACE $$ = $1; } | statement_list statement { TRACE $$ = newNode(';', 2, $1, $2); } ; ","3 Parsing syn-tax : the way in which words are put together to form phrases, clauses, or sentences. Webster’s Dictionary The abbreviation mechanism in Lex, whereby a symbol stands for some regu- lar expression, is convenient enough that it is tempting to use it in interesting ways: digits = [ 0 − 9 ]+ sum = ( digits “ + ”)* digits These regular expressions deﬁne sums of the form 28+301+9 . But now consider digits = [ 0 − 9 ]+ sum = expr “ + ” expr expr = “ ( ” sum “ ) ” | digits This is meant to deﬁne expressions of the form: (109+23) 61 (1+(250+3)) in which all the parentheses are balanced. But it is impossible for a ﬁnite au- tomaton to recognize balanced parentheses (because a machine with N states cannot remember a parenthesis-nesting depth greater than N ), so clearly sum and expr cannot be regular expressions. So how does Lex manage to implement regular-expression abbreviations such as digits ? The answer is that the right-hand-side ([0-9]+) is simply 39 ","CHAPTER THREE. PARSING substituted for digits wherever it appears in regular expressions, before translation to a ﬁnite automaton. This is not possible for the sum -and- expr language; we can ﬁrst substitute sum into expr , yielding expr = “ ( ” expr “ + ” expr “ ) ” | digits but now an attempt to substitute expr into itself leads to expr = “ ( ” ( “ ( ” expr “ + ” expr “ ) ” | digits ) “ + ” expr “ ) ” | digits and the right-hand side now has just as many occurrences of expr as it did before – in fact, it has more! Thus, the notion of abbreviation does not add expressive power to the lan- guage of regular expressions – there are no additional languages that can be deﬁned – unless the abbreviations are recursive (or mutually recursive, as are sum and expr ). The additional expressive power gained by recursion is just what we need for parsing. Also, once we have abbreviations with recursion, we do not need alternation except at the top level of expressions, because the deﬁnition expr = ab ( c | d ) e can always be rewritten using an auxiliary deﬁnition as aux = c | d expr = a b aux e In fact, instead of using the alternation mark at all, we can just write several allowable expansions for the same symbol: aux = c aux = d expr = a b aux e The Kleene closure is not necessary, since we can rewrite it so that expr = ( a b c ) ∗ becomes expr = ( a b c ) expr expr = ϵ 40 ","3.1. CONTEXT-FREE GRAMMARS 1 S → S ; S 2 S → id := E 3 S → print ( L ) 4 E → id 5 E → num 6 E → E + E 7 E → ( S , E ) 8 L → E 9 L → L , E GRAMMAR 3.1. A syntax for straight-line programs. What we have left is a very simple notation, called context-free grammars . Just as regular expressions can be used to deﬁne lexical structure in a static, declarative way, grammars deﬁne syntactic structure declaratively. But we will need something more powerful than ﬁnite automata to parse languages described by grammars. In fact, grammars can also be used to describe the structure of lexical to- kens, although regular expressions are adequate – and more concise – for that purpose. 3.1 CONTEXT-FREE GRAMMARS As before, we say that a language is a set of strings ; each string is a ﬁnite sequence of symbols taken from a ﬁnite alphabet . For parsing, the strings are source programs, the symbols are lexical tokens, and the alphabet is the set of token types returned by the lexical analyzer. A context-free grammar describes a language. A grammar has a set of productions of the form symbol → symbol symbol · · · symbol where there are zero or more symbols on the right-hand side. Each symbol is either terminal , meaning that it is a token from the alphabet of strings in the language, or nonterminal , meaning that it appears on the left-hand side of some production. No token can ever appear on the left-hand side of a produc- tion. Finally, one of the nonterminals is distinguished as the start symbol of the grammar. Grammar 3.1 is an example of a grammar for straight-line programs. The start symbol is S (when the start symbol is not written explicitly it is conven- tional to assume that the left-hand nonterminal in the ﬁrst production is the start symbol). The terminal symbols are id print num , + ( ) := ; 41 ","CHAPTER THREE. PARSING S S ; S S ; id := E id := E ; id := E id := num ; id := E id := num ; id := E + E id := num ; id := E + ( S , E ) id := num ; id := id + ( S , E ) id := num ; id := id + ( id := E , E ) id := num ; id := id + ( id := E + E , E ) id := num ; id := id + ( id := E + E , id ) id := num ; id := id + ( id := num + E , id ) id := num ; id := id + ( id := num + num , id ) DERIVATION 3.2. and the nonterminals are S , E , and L . One sentence in the language of this grammar is id := num; id := id + (id := num + num, id) where the source text (before lexical analysis) might have been a := 7; b := c + (d := 5 + 6, d) The token-types (terminal symbols) are id , num , := , and so on; the names ( a,b,c,d ) and numbers (7, 5, 6) are semantic values associated with some of the tokens. DERIVATIONS To show that this sentence is in the language of the grammar, we can per- form a derivation : start with the start symbol, then repeatedly replace any nonterminal by one of its right-hand sides, as shown in Derivation 3.2. There are many different derivations of the same sentence. A leftmost derivation is one in which the leftmost nonterminal symbol is always the one expanded; in a rightmost derivation, the rightmost nonterminal is always next to be expanded. 42 ","3.1. CONTEXT-FREE GRAMMARS . S S id := E num ; S id := E E id + E ( S id := E E num + E num , E id ) FIGURE 3.3. Parse tree. Derivation 3.2 is neither leftmost nor rightmost; a leftmost derivation for this sentence would begin, S S ; S id := E ; S id := num ; S id := num ; id := E id := num ; id := E + E . . . PARSE TREES A parse tree is made by connecting each symbol in a derivation to the one from which it was derived, as shown in Figure 3.3 . Two different derivations can have the same parse tree. AMBIGUOUS GRAMMARS A grammar is ambiguous if it can derive a sentence with two different parse trees. Grammar 3.1 is ambiguous, since the sentence id := id+id+id has two parse trees ( Figure 3.4 ). Grammar 3.5 is also ambiguous; Figure 3.6 shows two parse trees for the sentence 1-2-3 , and Figure 3.7 shows two trees for 1+2*3 . Clearly, if we use 43 ","CHAPTER THREE. PARSING . S id := E E E id + E id + E id . S id := E E id + E E id + E id FIGURE 3.4. Two parse trees for the same sentence using Grammar 3.1. E → id E → num E → E ∗ E E → E / E E → E + E E → E − E E → ( E ) GRAMMAR 3.5. . E E E 1 - E 2 - E 3 . E E 1 - E E 2 - E 3 FIGURE 3.6. Two parse trees for the sentence 1-2-3 in Grammar 3.5. . E E E 1 + E 2 * E 3 . E E 1 + E E 2 * E 3 FIGURE 3.7. Two parse trees for the sentence 1+2*3 in Grammar 3.5. 44 ","3.1. CONTEXT-FREE GRAMMARS E → E + T E → E − T E → T T → T ∗ F T → T / F T → F F → id F → num F → ( E ) GRAMMAR 3.8. . ?X + ?Y + . ?U ?V + * FIGURE 3.9. Parse trees that Grammar 3.8 will never produce. parse trees to interpret the meaning of the expressions, the two parse trees for 1-2-3 mean different things: ( 1 − 2 ) − 3 = − 4 versus 1 − ( 2 − 3 ) = 2. Similarly, ( 1 + 2 ) × 3 is not the same as 1 + ( 2 × 3 ) . And indeed, compilers do use parse trees to derive meaning. Therefore, ambiguous grammars are problematic for compiling: in general we would prefer to have unambiguous grammars. Fortunately, we can often transform ambiguous grammars to unambiguous grammars. Let us ﬁnd an unambigous grammar that accepts the same language as Grammar 3.5. First, we would like to say that * binds tighter than + , or has higher precedence . Second, we want to say that each operator associates to the left , so that we get ( 1 − 2 ) − 3 instead of 1 − ( 2 − 3 ) . We do this by introducing new nonterminal symbols to get Grammar 3.8. The symbols E , T , and F stand for expression , term , and factor ; conven- tionally, factors are things you multiply and terms are things you add. This grammar accepts the same set of sentences as the ambiguous gram- mar, but now each sentence has exactly one parse tree. Grammar 3.8 can never produce parse trees of the form shown in Figure 3.9 (see Exercise 3.17 ). Had we wanted to make * associate to the right, we could have written its production as T → F ∗ T . We can usually eliminate ambiguity by transforming the grammar. Though there are some languages (sets of strings) that have ambiguous grammars but no unambiguous grammar, such languages may be problematic as pro- gramming languages because the syntactic ambiguity may lead to problems in writing and understanding programs. 45 ","CHAPTER THREE. PARSING S → E $ E → E + T E → E − T E → T T → T ∗ F T → T / F T → F F → id F → num F → ( E ) GRAMMAR 3.10. S → if E then S else S S → begin S L S → print E L → end L → ; S L E → num = num GRAMMAR 3.11. END-OF-FILE MARKER Parsers must read not only terminal symbols such as + , - , num , and so on, but also the end-of-ﬁle marker. We will use $ to represent end of ﬁle. Suppose S is the start symbol of a grammar. To indicate that $ must come after a complete S -phrase, we augment the grammar with a new start symbol S ′ and a new production S ′ → S $. In Grammar 3.8, E is the start symbol, so an augmented grammar is Gram- mar 3.10. 3.2 PREDICTIVE PARSING Some grammars are easy to parse using a simple algorithm known as recur- sive descent . In essence, each grammar production turns into one clause of a recursive function. We illustrate this by writing a recursive-descent parser for Grammar 3.11. A recursive-descent parser for this language has one function for each non- terminal and one clause for each production. 46 ","3.2. PREDICTIVE PARSING enum token {IF, THEN, ELSE, BEGIN, END, PRINT, SEMI, NUM, EQ}; extern enum token getToken(void); enum token tok; void advance() {tok=getToken();} void eat(enum token t) {if (tok==t) advance(); else error();} void S(void) {switch(tok) { case IF: eat(IF); E(); eat(THEN); S(); eat(ELSE); S(); break; case BEGIN: eat(BEGIN); S(); L(); break; case PRINT: eat(PRINT); E(); break; default: error(); }} void L(void) {switch(tok) { case END: eat(END); break; case SEMI: eat(SEMI); S(); L(); break; default: error(); }} void E(void) { eat(NUM); eat(EQ); eat(NUM); } With suitable deﬁnitions of error and getToken , this program will parse very nicely. Emboldened by success with this simple method, let us try it with Gram- mar 3.10: void S(void) { E(); eat(EOF); } void E(void) {switch (tok) { case ?: E(); eat(PLUS); T(); break; case ?: E(); eat(MINUS); T(); break; case ?: T(); break; default: error(); }} void T(void) {switch (tok) { case ?: T(); eat(TIMES); F(); break; case ?: T(); eat(DIV); F(); break; case ?: F(); break; default: error(); }} There is a con°ict here: the E function has no way to know which clause to use. Consider the strings (1*2-3)+4 and (1*2-3) . In the former case, the initial call to E should use the E → E + T production, but the latter case should use E → T . Recursive-descent, or predictive , parsing works only on grammars where the ﬁrst terminal symbol of each subexpression provides enough information 47 ","CHAPTER THREE. PARSING Z → d Z → X Y Z Y → Y → c X → Y X → a GRAMMAR 3.12. to choose which production to use. To understand this better, we will formal- ize the notion of FIRST sets, and then derive con°ict-free recursive-descent parsers using a simple algorithm. Just as lexical analyzers can be constructed from regular expressions, there are parser-generator tools that build predictive parsers. But if we are going to use a tool, then we might as well use one based on the more powerful LR(1) parsing algorithm, which will be described in Section 3.3 . Sometimes it’s inconvenient or impossible to use a parser-generator tool. The advantage of predictive parsing is that the algorithm is simple enough that we can use it to construct parsers by hand – we don’t need automatic tools. FIRST AND FOLLOW SETS Given a string γ of terminal and nonterminal symbols, FIRST ( γ ) is the set of all terminal symbols that can begin any string derived from γ . For example, let γ = T ∗ F . Any string of terminal symbols derived from γ must start with id , num , or ( . Thus, FIRST ( T ∗ F ) = { id , num , ( } . If two different productions X → γ 1 and X → γ 2 have the same left- hand-side symbol ( X ) and their right-hand sides have overlapping FIRST sets, then the grammar cannot be parsed using predictive parsing. If some terminal symbol I is in FIRST ( γ 1 ) and also in FIRST ( γ 2 ) , then the X func- tion in a recursive-descent parser will not know what to do if the input token is I . The computation of FIRST sets looks very simple: if γ = X Y Z , it seems as if Y and Z can be ignored, and FIRST ( X ) is the only thing that matters. But consider Grammar 3.12. Because Y can produce the empty string – and therefore X can produce the empty string – we ﬁnd that FIRST ( X Y Z ) must include FIRST ( Z ) . Therefore, in computing FIRST sets, we must keep track of which symbols can produce the empty string; we say such symbols are nullable . And we must keep track of what might follow a nullable symbol. With respect to a particular grammar, given a string γ of terminals and nonterminals, 48 ","3.2. PREDICTIVE PARSING • nullable ( X ) is true if X can derive the empty string. • FIRST ( γ ) is the set of terminals that can begin strings derived from γ . • FOLLOW ( X ) is the set of terminals that can immediately follow X . That is, t ∈ FOLLOW ( X ) if there is any derivation containing Xt . This can occur if the derivation contains X Y Zt where Y and Z both derive ϵ . A precise deﬁnition of FIRST, FOLLOW, and nullable is that they are the smallest sets for which these properties hold: For each terminal symbol Z , FIRST [ Z ] = { Z } . for each production X → Y 1 Y 2 · · · Y k for each i from 1 to k , each j from i + 1 to k , if all the Y i are nullable then nullable [ X ] = true if Y 1 · · · Y i − 1 are all nullable then FIRST [ X ] = FIRST [ X ] ∪ FIRST [ Y i ] if Y i + 1 · · · Y k are all nullable then FOLLOW [ Y i ] = FOLLOW [ Y i ] ∪ FOLLOW [ X ] if Y i + 1 · · · Y j − 1 are all nullable then FOLLOW [ Y i ] = FOLLOW [ Y i ] ∪ FIRST [ Y j ] Algorithm 3.13 for computing FIRST, FOLLOW, and nullable just follows from these facts; we simply replace each equation with an assignment state- ment, and iterate. Of course, to make this algorithm efﬁcient it helps to examine the produc- tions in the right order; see Section 17.4 . Also, the three relations need not be computed simultaneously; nullable can be computed by itself, then FIRST, then FOLLOW. This is not the ﬁrst time that a group of equations on sets has become the algorithm for calculating those sets; recall the algorithm on page 28 for computing ϵ -closure. Nor will it be the last time; the technique of iteration to a ﬁxed point is applicable in data°ow analysis for optimization, in the back end of a compiler. We can apply this algorithm to Grammar 3.12. Initially, we have: nullable FIRST FOLLOW X no Y no Z no In the ﬁrst iteration, we ﬁnd that a ∈ FIRST [ X ] , Y is nullable, c ∈ FIRST [ Y ] , d ∈ FIRST [ Z ] , d ∈ FOLLOW [ X ] , c ∈ FOLLOW [ X ] , 49 ","CHAPTER THREE. PARSING Algorithm to compute FIRST , FOLLOW , and nullable . Initialize FIRST and FOLLOW to all empty sets, and nullable to all false. for each terminal symbol Z FIRST [ Z ] ← { Z } repeat for each production X → Y 1 Y 2 · · · Y k for each i from 1 to k , each j from i + 1 to k , if all the Y i are nullable then nullable [ X ] ← true if Y 1 · · · Y i − 1 are all nullable then FIRST [ X ] ← FIRST [ X ] ∪ FIRST [ Y i ] if Y i + 1 · · · Y k are all nullable then FOLLOW [ Y i ] ← FOLLOW [ Y i ] ∪ FOLLOW [ X ] if Y i + 1 · · · Y j − 1 are all nullable then FOLLOW [ Y i ] ← FOLLOW [ Y i ] ∪ FIRST [ Y j ] until FIRST, FOLLOW, and nullable did not change in this iteration. ALGORITHM 3.13. Iterative computation of FIRST , FOLLOW , and nullable . d ∈ FOLLOW [ Y ] . Thus: nullable FIRST FOLLOW X no a c d Y yes c d Z no d In the second iteration, we ﬁnd that X is nullable, c ∈ FIRST [ X ] , { a , c } ⊆ FIRST [ Z ] , { a , c , d } ⊆ FOLLOW [ X ] , { a , c , d } ⊆ FOLLOW [ Y ] . Thus: nullable FIRST FOLLOW X yes a c a c d Y yes c a c d Z no a c d The third iteration ﬁnds no new information, and the algorithm terminates. It is useful to generalize the FIRST relation to strings of symbols: FIRST ( X γ ) = FIRST [ X ] if not nullable [ X ] FIRST ( X γ ) = FIRST [ X ] ∪ FIRST ( γ ) if nullable [ X ] 50 ","3.2. PREDICTIVE PARSING a c d X X → a X → Y X → Y X → Y Y Y → Y → Y → c Y → Z Z → XY Z Z → XY Z Z → d Z → XY Z FIGURE 3.14. Predictive parsing table for Grammar 3.12. and similarly, we say that a string γ is nullable if each symbol in γ is nullable. CONSTRUCTING A PREDICTIVE PARSER Consider a recursive-descent parser. The parsing function for some nontermi- nal X has a clause for each X -production; it must choose one of these clauses based on the next token T of the input. If we can choose the right produc- tion for each ( X , T ) , then we can write the recursive-descent parser. All the information we need can be encoded as a two-dimensional table of produc- tions, indexed by nonterminals X and terminals T . This is called a predictive parsing table. To construct this table, enter production X → γ in row X , column T of the table for each T ∈ FIRST ( γ ) . Also, if γ is nullable, enter the production in row X , column T for each T ∈ FOLLOW [ X ] . Figure 3.14 shows the predictive parser for Grammar 3.12. But some of the entries contain more than one production! The presence of duplicate entries means that predictive parsing will not work on Grammar 3.12. If we examine the grammar more closely, we ﬁnd that it is ambiguous. The sentence d has many parse trees, including: . Z d . Z X Y Y Z d An ambiguous grammar will always lead to duplicate entries in a predictive parsing table. If we need to use the language of Grammar 3.12 as a program- ming language, we will need to ﬁnd an unambiguous grammar. Grammars whose predictive parsing tables contain no duplicate entries 51 ","CHAPTER THREE. PARSING are called LL(1). This stands for Left-to-right parse, Leftmost-derivation, 1-symbol lookahead . Clearly a recursive-descent (predictive) parser exam- ines the input left-to-right in one pass (some parsing algorithms do not, but these are generally not useful for compilers). The order in which a predic- tive parser expands nonterminals into right-hand sides (that is, the recursive- descent parser calls functions corresponding to nonterminals) is just the order in which a leftmost derivation expands nonterminals. And a recursive-descent parser does its job just by looking at the next token of the input, never looking more than one token ahead. We can generalize the notion of FIRST sets to describe the ﬁrst k tokens of a string, and to make an LL( k ) parsing table whose rows are the nonterminals and columns are every sequence of k terminals. This is rarely done (because the tables are so large), but sometimes when you write a recursive-descent parser by hand you a need to look more than one token ahead. Grammars parsable with LL(2) parsing tables are called LL(2) grammars, and similarly for LL(3), etc. Every LL(1) grammar is an LL(2) grammar, and so on. No ambiguous grammar is LL( k ) for any k . ELIMINATING LEFT RECURSION Suppose we want to build a predictive parser for Grammar 3.10. The two productions E → E + T E → T are certain to cause duplicate entries in the LL(1) parsing table, since any token in FIRST ( T ) will also be in FIRST ( E + T ) . The problem is that E appears as the ﬁrst right-hand-side symbol in an E -production; this is called left recursion . Grammars with left recursion cannot be LL(1). To eliminate left recursion, we will rewrite using right recursion. We intro- duce a new nonterminal E ′ , and write E → T E ′ E ′ → + T E ′ E ′ → This derives the same set of strings (on T and + ) as the original two produc- tions, but now there is no left recursion. 52 ","3.2. PREDICTIVE PARSING S → E $ E → T E ′ E ′ → + T E ′ E ′ → − T E ′ E ′ → T → F T ′ T ′ → ∗ F T ′ T ′ → / F T ′ T ′ → F → id F → num F → ( E ) GRAMMAR 3.15. nullable FIRST FOLLOW S no ( id num E no ( id num ) $ E ′ yes + - ) $ T no ( id num ) + - $ T ′ yes * / ) + - $ F no ( id num ) * / + - $ TABLE 3.16. Nullable, FIRST, and FOLLOW for Grammar 3.8. In general, whenever we have productions X → X γ and X → α , where α does not start with X , we know that this derives strings of the form αγ ∗ , an α followed by zero or more γ . So we can rewrite the regular expression using right recursion: ⎛ ⎜ ⎜ ⎜ ⎝ X → X γ 1 X → X γ 2 X → α 1 X → α 2 ⎞ ⎟ ⎟ ⎟ ⎠ ±⇒ ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ X → α 1 X ′ X → α 2 X ′ X ′ → γ 1 X ′ X ′ → γ 2 X ′ X ′ → ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ Applying this transformation to Grammar 3.10, we obtain Grammar 3.15. To build a predictive parser, ﬁrst we compute nullable, FIRST, and FOL- LOW (Table 3.16) . The predictive parser for Grammar 3.15 is shown in Ta- ble 3.17. LEFT FACTORING We have seen that left recursion interferes with predictive parsing, and that it can be eliminated. A similar problem occurs when two productions for the 53 ","CHAPTER THREE. PARSING + * id ( ) $ S S → E $ S → E $ E E → T E ′ E → T E ′ E ′ E ′ → + T E ′ E ′ → E ′ → T T → FT ′ T → FT ′ T ′ T ′ → T ′ → ∗ FT ′ T ′ → T ′ → F F → id F → ( E ) TABLE 3.17. Predictive parsing table for Grammar 3.15. We omit the columns for num , / , and - , as they are similar to others in the table. same nonterminal start with the same symbols. For example: S → if E then S else S S → if E then S In such a case, we can left factor the grammar – that is, take the allowable endings (“else S ” and ϵ ) and make a new nonterminal X to stand for them: S → if E then S X X → X → else S The resulting productions will not pose a problem for a predictive parser. ERROR RECOVERY Armed with a predictive parsing table, it is easy to write a recursive-descent parser. Here is a representative fragment of a parser for Grammar 3.15: void T(void) {switch (tok) { case ID: case NUM: case LPAREN: F(); Tprime(); break; default: error! }} void Tprime(void) {switch (tok) { case PLUS: break; case TIMES: eat(TIMES); F(); Tprime(); break; case EOF: break; case RPAREN: break; default: error! }} 54 ","3.2. PREDICTIVE PARSING A blank entry in row T , column x of the LL(1) parsing table indicates that the parsing function T() does not expect to see token x – this will be a syntax error. How should error be handled? It is safe just to raise an exception and quit parsing, but this is not very friendly to the user. It is better to print an error message and recover from the error, so that other syntax errors can be found in the same compilation. A syntax error occurs when the string of input tokens is not a sentence in the language. Error recovery is a way of ﬁnding some sentence similar to that string of tokens. This can proceed by deleting, replacing, or inserting tokens. For example, error recovery for T could proceed by inserting a num token. It’s not necessary to adjust the actual input; it sufﬁces to pretend that the num was there, print a message, and return normally. void T(void) {switch (tok) { case ID: case NUM: case LPAREN: F(); Tprime(); break; default: printf(expected id, num, or left-paren); }} It’s a bit dangerous to do error recovery by insertion, because if the error cascades to produce another error, the process might loop inﬁnitely. Error re- covery by deletion is safer, because the loop must eventually terminate when end-of-ﬁle is reached. Simple recovery by deletion works by skipping tokens until a token in the FOLLOW set is reached. For example, error recovery for T ′ could work like this: int Tprime_follow [] = {PLUS, TIMES, RPAREN, EOF, -1}; void Tprime(void) { switch (tok) { case PLUS: break; case TIMES: eat(TIMES); F(); Tprime(); break; case RPAREN: break; case EOF: break; default: printf(expected +, *, right-paren, or end-of-file); skipto(Tprime_follow); }} A recursive-descent parser’s error-recovery mechanisms must be adjusted (sometimes by trial and error) to avoid a long cascade of error-repair mes- sages resulting from a single token out of place. 55 ","CHAPTER THREE. PARSING 3.3 LR PARSING The weakness of LL( k ) parsing techniques is that they must predict which production to use, having seen only the ﬁrst k tokens of the right-hand side. A more powerful technique, LR( k ) parsing, is able to postpone the decision until it has seen input tokens corresponding to the entire right-hand side of the production in question (and k more input tokens beyond). LR( k ) stands for Left-to-right parse, Rightmost-derivation, k-token looka- head. The use of a rightmost derivation seems odd; how is that compatible with a left-to-right parse? Figure 3.18 illustrates an LR parse of the program a := 7; b := c + (d := 5 + 6, d) using Grammar 3.1, augmented with a new start production S ′ → S $. The parser has a stack and an input . The ﬁrst k tokens of the input are the lookahead . Based on the contents of the stack and the lookahead, the parser performs two kinds of actions: Shift: move the ﬁrst input token to the top of the stack. Reduce: Choose a grammar rule X → A B C ; pop C , B , A from the top of the stack; push X onto the stack. Initially, the stack is empty and the parser is at the beginning of the input. The action of shifting the end-of-ﬁle marker $ is called accepting and causes the parser to stop successfully. In Figure 3.18 , the stack and input are shown after every step, along with an indication of which action has just been performed. The concatenation of stack and input is always one line of a rightmost derivation; in fact, Fig- ure 3.18 shows the rightmost derivation of the input string, upside-down. LR PARSING ENGINE How does the LR parser know when to shift and when to reduce? By us- ing a deterministic ﬁnite automaton! The DFA is not applied to the input – ﬁnite automata are too weak to parse context-free grammars – but to the stack. The edges of the DFA are labeled by the symbols (terminals and non- terminals) that can appear on the stack. Table 3.19 is the transition table for Grammar 3.1. The elements in the transition table are labeled with four kinds of actions: 56 ","3.3. LR PARSING Stack Input Action 1 a := 7 ; b := c + ( d := 5 + 6 , d ) $ shift 1 id 4 := 7 ; b := c + ( d := 5 + 6 , d ) $ shift 1 id 4 := 6 7 ; b := c + ( d := 5 + 6 , d ) $ shift 1 id 4 := 6 num 10 ; b := c + ( d := 5 + 6 , d ) $ reduce E → num 1 id 4 := 6 E 11 ; b := c + ( d := 5 + 6 , d ) $ reduce S → id := E 1 S 2 ; b := c + ( d := 5 + 6 , d ) $ shift 1 S 2 ; 3 b := c + ( d := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := c + ( d := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 c + ( d := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 id 20 + ( d := 5 + 6 , d ) $ reduce E → id 1 S 2 ; 3 id 4 := 6 E 11 + ( d := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( d := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 d := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 5 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 num 10 + 6 , d ) $ reduce E → num 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 E 11 + 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 E 11 + 16 6 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 E 11 + 16 num 10 , d ) $ reduce E → num 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 E 11 + 16 E 17 , d ) $ reduce E → E + E 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 id 4 := 6 E 11 , d ) $ reduce S → id := E 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 S 12 , d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 S 12 , 18 d ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 S 12 , 18 id 20 ) $ reduce E → id 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 S 12 , 18 E 21 ) $ shift 1 S 2 ; 3 id 4 := 6 E 11 + 16 ( 8 S 12 , 18 E 21 ) 22 $ reduce E → ( S , E ) 1 S 2 ; 3 id 4 := 6 E 11 + 16 E 17 $ reduce E → E + E 1 S 2 ; 3 id 4 := 6 E 11 $ reduce S → id := E 1 S 2 ; 3 S 5 $ reduce S → S ; S 1 S 2 $ accept FIGURE 3.18. Shift-reduce parse of a sentence. Numeric subscripts in the Stack are DFA state numbers; see Table 3.19 . s n Shift into state n ; g n Goto state n ; r k Reduce by rule k ; a Accept; Error (denoted by a blank entry in the table). To use this table in parsing, treat the shift and goto actions as edges of a DFA, and scan the stack. For example, if the stack is id := E , then the DFA goes from state 1 to 4 to 6 to 11. If the next input token is a semicolon, then the “;” column in state 11 says to reduce by rule 2. The second rule of the 57 ","CHAPTER THREE. PARSING id num print ; , + := ( ) $ S E L 1 s4 s7 g2 2 s3 a 3 s4 s7 g5 4 s6 5 r1 r1 r1 6 s20 s10 s8 g11 7 s9 8 s4 s7 g12 9 g15 g14 10 r5 r5 r5 r5 r5 11 r2 r2 s16 r2 12 s3 s18 13 r3 r3 r3 14 s19 s13 15 r8 r8 16 s20 s10 s8 g17 17 r6 r6 s16 r6 r6 18 s20 s10 s8 g21 19 s20 s10 s8 g23 20 r4 r4 r4 r4 r4 21 s22 22 r7 r7 r7 r7 r7 23 r9 s16 r9 TABLE 3.19. LR parsing table for Grammar 3.1. 0 S ′ → S $ 1 S → ( L ) 2 S → x 3 L → S 4 L → L , S GRAMMAR 3.20. grammar is S → id := E , so the top three tokens are popped from the stack and S is pushed. The action for “+” in state 11 is to shift; so if the next token had been + instead, it would have been eaten from the input and pushed on the stack. Rather than rescan the stack for each token, the parser can remember in- stead the state reached for each stack element. Then the parsing algorithm is: 58 ","3.3. LR PARSING Look up top stack state, and input symbol, to get action; If action is Shift( n ): Advance input one token; push n on stack. Reduce( k ): Pop stack as many times as the number of symbols on the right-hand side of rule k ; Let X be the left-hand-side symbol of rule k ; In the state now on top of stack, look up X to get “goto n ”; Push n on top of stack. Accept: Stop parsing, report success. Error: Stop parsing, report failure. LR(0) PARSER GENERATION An LR( k ) parser uses the contents of its stack and the next k tokens of the input to decide which action to take. Table 3.19 shows the use of one sym- bol of lookahead. For k = 2, the table has columns for every two-token se- quence and so on; in practice, k &gt; 1 is not used for compilation. This is partly because the tables would be huge, but more because most reasonable programming languages can be described by L R ( 1 ) grammars. LR(0) grammars are those that can be parsed looking only at the stack, making shift/reduce decisions without any lookahead. Though this class of grammars is too weak to be very useful, the algorithm for constructing LR(0) parsing tables is a good introduction to the LR(1) parser construction algo- rithm. We will use Grammar 3.20 to illustrate LR(0) parser generation. Consider what the parser for this grammar will be doing. Initially, it will have an empty stack, and the input will be a complete S -sentence followed by $; that is, the right-hand side of the S ′ rule will be on the input. We indicate this as S ′ → . S $ where the dot indicates the current position of the parser. In this state, where the input begins with S , that means that it begins with any possible right-hand side of an S -production; we indicate that by S ′ → . S $ S → . x S → .( L ) 1 Call this state 1. A grammar rule, combined with the dot that indicates a position in its right-hand side, is called an item (speciﬁcally, an LR(0) item ). A state is just a set of items. 59 ","CHAPTER THREE. PARSING Shift actions. In state 1, consider what happens if we shift an x . We then know that the end of the stack has an x ; we indicate that by shifting the dot past the x in the S → x production. The rules S ′ → . S $ and S → .( L ) are irrelevant to this action, so we ignore them; we end up in state 2: S → x . 2 Or in state 1 consider shifting a left parenthesis. Moving the dot past the parenthesis in the third item yields S → (. L ) , where we know that there must be a left parenthesis on top of the stack, and the input begins with some string derived by L , followed by a right parenthesis. What tokens can begin the input now? We ﬁnd out by including all L -productions in the set of items. But now, in one of those L -items, the dot is just before an S , so we need to include all the S -productions: S → (. L ) L → . L , S L → . S S → .( L ) S → . x 3 Goto actions. In state 1, consider the effect of parsing past some string of to- kens derived by the S nonterminal. This will happen when an x or left paren- thesis is shifted, followed (eventually) by a reduction of an S -production. All the right-hand-side symbols of that production will be popped, and the parser will execute the goto action for S in state 1. The effect of this can be simulated by moving the dot past the S in the ﬁrst item of state 1, yielding state 4: S ′ → S . $ 4 Reduce actions. In state 2 we ﬁnd the dot at the end of an item. This means that on top of the stack there must be a complete right-hand side of the cor- responding production ( S → x ), ready to reduce. In such a state the parser could perform a reduce action. The basic operations we have been performing on states are closure ( I ) , and goto ( I , X ) , where I is a set of items and X is a grammar symbol (terminal or nonterminal). Closure adds more items to a set of items when there is a dot to the left of a nonterminal; goto moves the dot past the symbol X in all items. 60 ","3.3. LR PARSING Closure ( I ) = repeat for any item A → α . X β in I for any production X → γ I ← I ∪ { X → . γ } until I does not change. return I Goto ( I , X ) = set J to the empty set for any item A → α . X β in I add A → α X . β to J return Closure ( J ) Now here is the algorithm for LR(0) parser construction. First, augment the grammar with an auxiliary start production S ′ → S $. Let T be the set of states seen so far, and E the set of (shift or goto) edges found so far. Initialize T to { Closure ( { S ′ → . S $ } ) } Initialize E to empty. repeat for each state I in T for each item A → α . X β in I let J be Goto ( I , X ) T ← T ∪ { J } E ← E ∪ { I X → J } until E and T did not change in this iteration However, for the symbol $ we do not compute Goto ( I , $ ) ; instead we will make an accept action. For Grammar 3.20 this is illustrated in Figure 3.21 . Now we can compute set R of LR(0) reduce actions: R ← {} for each state I in T for each item A → α . in I R ← R ∪ { ( I , A → α ) } We can now construct a parsing table for this grammar (Table 3.22) . For each edge I X → J where X is a terminal, we put the action shift J at position ( I , X ) of the table; if X is a nonterminal we put goto J at position ( I , X ) . For each state I containing an item S ′ → S . $ we put an accept action at ( I , $ ) . Finally, for a state containing an item A → γ . (production n with the dot at the end), we put a reduce n action at ( I , Y ) for every token Y . In principle, since LR(0) needs no lookahead, we just need a single action for each state: a state will shift or reduce, but not both. In practice, since we 61 ","CHAPTER THREE. PARSING S ' . S $ S . ( L ) S . x S ' S . $ S x . S ( . L ) L . S L . L , S S . ( L ) S . x L S . L L , . S S . ( L ) S . x S ( L . ) L L . , S S ( L ) . L L , S . S x ( ( S x ( L ) , S 1 2 3 4 5 6 7 8 9 x FIGURE 3.21. LR(0) states for Grammar 3.20. ( ) x , $ S L 1 s3 s2 g4 2 r2 r2 r2 r2 r2 3 s3 s2 g7 g5 4 a 5 s6 s8 6 r1 r1 r1 r1 r1 7 r3 r3 r3 r3 r3 8 s3 s2 g9 9 r4 r4 r4 r4 r4 TABLE 3.22. LR(0) parsing table for Grammar 3.20. 0 S → E $ 1 E → T + E 2 E → T 3 T → x GRAMMAR 3.23. need to know what state to shift into, we have rows headed by state numbers and columns headed by grammar symbols. SLR PARSER GENERATION Let us attempt to build an LR(0) parsing table for Grammar 3.23. The LR(0) states and parsing table are shown in Figure 3.24 . In state 3, on symbol + , there is a duplicate entry: the parser must shift into state 4 and also reduce by production 2. This is a con°ict and indicates that 62 ","3.3. LR PARSING T x . S E . $ E T . + E E T . x E T E + 1 2 3 5 4 E T + E . 6 E T + . E E . T + E E . T T . x S . E $ E . T + E E . T T . x x T x + $ E T 1 s5 g2 g3 2 a 3 r2 s4,r2 r2 4 s5 g6 g3 5 r3 r3 r3 6 r1 r1 r1 FIGURE 3.24. LR(0) states and parsing table for Grammar 3.23. the grammar is not LR(0) – it cannot be parsed by an LR(0) parser. We will need a more powerful parsing algorithm. A simple way of constructing better-than-LR(0) parsers is called SLR, which stands for Simple LR. Parser construction for SLR is almost identi- cal to that for LR(0), except that we put reduce actions into the table only where indicated by the FOLLOW set. Here is the algorithm for putting reduce actions into an SLR table: R ← {} for each state I in T for each item A → α . in I for each token X in FOLLOW ( A ) R ← R ∪ { ( I , X , A → α ) } The action ( I , X , A → α ) indicates that in state I , on lookahead symbol X , the parser will reduce by rule A → α . Thus, for Grammar 3.23 we use the same LR(0) state diagram ( Figure 3.24 ), but we put fewer reduce actions into the SLR table, as shown in Figure 3.25. The SLR class of grammars is precisely those grammars whose SLR pars- ing table contains no con°icts (duplicate entries). Grammar 3.23 belongs to this class, as do many useful programming-language grammars. 63 ","CHAPTER THREE. PARSING x + $ E T 1 s5 g2 g3 2 a 3 s4 r2 4 s5 g6 g3 5 r3 r3 6 r1 FIGURE 3.25. SLR parsing table for Grammar 3.23. LR(1) ITEMS; LR(1) PARSING TABLE Even more powerful than SLR is the LR(1) parsing algorithm. Most program- ming languages whose syntax is describable by a context-free grammar have an LR(1) grammar. The algorithm for constructing an LR(1) parsing table is similar to that for LR(0), but the notion of an item is more sophisticated. An LR(1) item consists of a grammar production , a right-hand-side position (represented by the dot), and a lookahead symbol . The idea is that an item ( A → α . β , x ) indicates that the sequence α is on top of the stack, and at the head of the input is a string derivable from β x . An LR(1) state is a set of LR(1) items, and there are Closure and Goto operations for LR(1) that incorporate the lookahead: Closure ( I ) = repeat for any item ( A → α . X β , z ) in I for any production X → γ for any w ∈ FIRST ( β z ) I ← I ∪ { ( X → . γ , w) } until I does not change return I Goto ( I , X ) = J ← { } for any item ( A → α . X β , z ) in I add ( A → α X . β , z ) to J return Closure ( J ) . The start state is the closure of the item ( S ′ → . S $ , ? ) , where the looka- head symbol ? will not matter, because the end-of-ﬁle marker will never be shifted. The reduce actions are chosen by this algorithm: R ← {} for each state I in T for each item ( A → α . , z ) in I R ← R ∪ { ( I , z , A → α ) } 64 ","3.3. LR PARSING The action ( I , z , A → α ) indicates that in state I , on lookahead symbol z , the parser will reduce by rule A → α . Grammar 3.26 is not SLR (see Exercise 3.9 ), but it is in the class of LR(1) grammars. Figure 3.27 shows the LR(1) states for this grammar; in the ﬁgure, where there are several items with the same production but different looka- head, as at left below, I have abbreviated as at right: S ′ → . S &amp; ? S ′ → . S &amp; ? S → . V = E $ S → . V = E $ S → . E $ S → . E $ E → . V $ E → . V $ V → . x $ V → . x $ , = V → . * E $ V → . * E $ , = V → . x = V → . * E = The LR(1) parsing table derived from this state graph is Table 3.28a . Wher- ever the dot is at the end of a production (as in state 3 of Figure 3.27 , where it is at the end of production E → V ), then there is a reduce action for that production in the LR(1) table, in the row corresponding to the state number and the column corresponding to the lookahead of the item (in this case, the lookahead is $). Whenever the dot is to the left of a terminal symbol or non- terminal, there is a corresponding shift or goto action in the LR(1) parsing table, just as there would be in an LR(0) table. LALR(1) PARSING TABLES LR(1) parsing tables can be very large, with many states. A smaller table can be made by merging any two states whose items are identical except for lookahead sets. The result parser is called an LALR(1) parser, for Look-Ahead LR(1). For example, the items in states 6 and 13 of the LR(1) parser for Gram- mar 3.26 ( Figure 3.27 ) are identical if the lookahead sets are ignored. Also, states 7 and 12 are identical except for lookahead, as are states 8 and 11 and states 10 and 14. Merging these pairs of states gives the LALR(1) parsing table shown in Figure 3.28b. For some grammars, the LALR(1) table contains reduce-reduce con°icts where the LR(1) table has none, but in practice the difference matters little. 65 ","CHAPTER THREE. PARSING 0 S ′ → S $ 1 S → V = E 2 S → E 3 E → V 4 V → x 5 V → * E GRAMMAR 3.26. A grammar capturing the essence of expressions, variables, and pointer-dereference (by the * ) operator in the C language. S ' S . $ ? 2 S ' . S $ S . V = E S . E E V ? $ $ $ $,= 1 V *. E E . V V x T . *E $,= V . .V x .* E $,= . $,= $,= $,= 6 S V . = E E V . $ $ S E . $ 3 5 V E E V . $ V x . $,= V x . $ E V . $.= V *E . $.= x x 7 8 11 12 10 S V = . E E . V V . x V $ $ $ $ . * E S V = E . $ 9 V * . E E . V V . x V $ $ $ $ . * E * V * E . $ = V x x 4 13 14 * E E * V E V FIGURE 3.27. LR(1) states for Grammar 3.26. x * = $ S E V 1 s8 s6 g2 g5 g3 2 a 3 s4 r3 4 s11 s13 g9 g7 5 r2 6 s8 s6 g10 g12 7 r3 8 r4 r4 9 r1 10 r5 r5 11 r4 12 r3 r3 13 s11 s13 g14 g7 14 r5 (a) LR(1) x * = $ S E V 1 s8 s6 g2 g5 g3 2 a 3 s4 r3 4 s8 s6 g9 g7 5 r2 6 s8 s6 g10 g7 7 r3 r3 8 r4 r4 9 r1 10 r5 r5 (b) LALR(1) TABLE 3.28. LR(1) and LALR(1) parsing tables for Grammar 3.26. 66 ","3.3. LR PARSING Unambiguous Grammars LL(0) LL(1) LL(k) LR(0) SLR LALR(1) LR(1) LR(k) Ambiguous Grammars FIGURE 3.29. A hierarchy of grammar classes. What does matter is that the LALR(1) parsing table requires less memory to represent than the LR(1) table, since there can be many fewer states. HIERARCHY OF GRAMMAR CLASSES A grammar is said to be LALR(1) if its LALR(1) parsing table contains no con°icts. All SLR grammars are LALR(1), but not vice versa. Figure 3.29 shows the relationship between several classes of grammars. Any reasonable programming language has a LALR(1) grammar, and there are many parser-generator tools available for LALR(1) grammars. For this reason, LALR(1) has become a standard for programming languages and for automatic parser generators. 67 ","CHAPTER THREE. PARSING LR PARSING OF AMBIGUOUS GRAMMARS Many programming languages have grammar rules such as S → if E then S else S S → if E then S S → other which allow programs such as if a then if b then s1 else s2 Such a program could be understood in two ways: (1) if a then { if b then s1 else s2 } (2) if a then { if b then s1 } else s2 In most programming languages, an else must match the most recent pos- sible then , so interpretation (1) is correct. In the LR parsing table there will be a shift-reduce con°ict: S → if E then S . else S → if E then S . else S ( any ) Shifting corresponds to interpretation (1) and reducing to interpretation (2) . The ambiguity can be eliminated by introducing auxiliary nonterminals M (for matched statement ) and U (for unmatched statement ): S → M S → U M → if E then M else M M → other U → if E then S U → if E then M else U But instead of rewriting the grammar, we can leave the grammar unchanged and tolerate the shift-reduce con°ict. In constructing the parsing table this con°ict should be resolved by shifting, since we prefer interpretation (1) . It is often possible to use ambiguous grammars by resolving shift-reduce con°icts in favor of shifting or reducing, as appropriate. But it is best to use this technique sparingly, and only in cases (such as the dangling-else de- scribed here, and operator-precedence to be described on page 73) that are 68 ","3.4. USING PARSER GENERATORS 1 P → L 2 S → id := id 3 S → while id do S 4 S → begin L end 5 S → if id then S 6 S → if id then S else S 7 L → S 8 L → L ; S GRAMMAR 3.30. well understood. Most shift-reduce con°icts, and probably all reduce-reduce con°icts, should not be resolved by ﬁddling with the parsing table. They are symptoms of an ill-speciﬁed grammar, and they should be resolved by elimi- nating ambiguities. 3.4 USING PARSER GENERATORS The task of constructing LR(1) or LALR(1) parsing tables is simple enough to be automated. And it is so tedious to do by hand that LR parsing for real- istic grammars is rarely done except using parser-generator tools. Yacc (“Yet another compiler-compiler”) is a classic and widely used parser generator; Bison and occs are more recent implementations. A Yacc speciﬁcation is divided into three sections, separated by %% marks: parser declarations %% grammar rules %% programs The parser declarations include a list of the terminal symbols, nonterminals, and so on. The programs are ordinary C code usable from the semantic ac- tions embedded in the earlier sections. The grammar rules are productions of the form exp : exp PLUS exp { semantic action } where exp is a nonterminal producing a right-hand side of exp+exp , and PLUS is a terminal symbol (token). The semantic action is written in ordinary C and will be executed whenever the parser reduces using this rule. 69 ","CHAPTER THREE. PARSING %{ int yylex(void); void yyerror(char *s) { EM_error(EM_tokPos, %s, s); } %} %token ID WHILE BEGIN END DO IF THEN ELSE SEMI ASSIGN %start prog %% prog: stmlist stm : ID ASSIGN ID | WHILE ID DO stm | BEGIN stmlist END | IF ID THEN stm | IF ID THEN stm ELSE stm stmlist : stm | stmlist SEMI stm GRAMMAR 3.31. Yacc version of Grammar 3.30. Semantic actions are omitted and will be discussed in Chapter 4 . Consider Grammar 3.30. It can be encoded in Yacc as shown in Gram- mar 3.31. The Yacc manual gives a complete explanation of the directives in a grammar speciﬁcation; in this grammar, the terminal symbols are ID , WHILE , etc.; the nonterminals are prog , stm , stmlist ; and the grammar’s start symbol is prog . CONFLICTS Yacc reports shift-reduce and reduce-reduce con°icts. A shift-reduce con°ict is a choice between shifting and reducing; a reduce-reduce con°ict is a choice of reducing by two different rules. By default, Yacc resolves shift-reduce con- °icts by shifting, and reduce-reduce con°icts by using the rule that appears earlier in the grammar. Yacc will report that this Grammar 3.30 has a shift-reduce con°ict. Any con°ict is cause for concern, because it may indicate that the parse will not be as the grammar-designer expected. The con°ict can be examined by reading the verbose description ﬁle that Yacc produces. Figure 3.32 shows this ﬁle. A brief examination of state 17 reveals that the con°ict is caused by the familiar dangling else . Since Yacc’s default resolution of shift-reduce con- °icts is to shift, and shifting gives the desired result of binding an else to the nearest then , this con°ict is not harmful. 70 ","3.4. USING PARSER GENERATORS state 0: prog : . stmlist ID shift 6 WHILE shift 5 BEGIN shift 4 IF shift 3 prog goto 21 stm goto 2 stmlist goto 1 . error state 1: prog : stmlist . stmlist : stmlist . SEMI stm SEMI shift 7 . reduce by rule 0 state 2: stmlist : stm . . reduce by rule 6 state 3: stm : IF . ID THEN stm stm : IF . ID THEN stm ELSE stm ID shift 8 . error state 4: stm : BEGIN . stmlist END ID shift 6 WHILE shift 5 BEGIN shift 4 IF shift 3 stm goto 2 stmlist goto 9 . error state 5: stm : WHILE . ID DO stm ID shift 10 . error state 6: stm : ID . ASSIGN ID ASSIGNshift 11 . error state 7: stmlist : stmlist SEMI . stm ID shift 6 WHILE shift 5 BEGIN shift 4 IF shift 3 stm goto 12 . error state 8: stm : IF ID . THEN stm stm : IF ID . THEN stm ELSE stm THEN shift 13 . error state 9: stm : BEGIN stmlist . END stmlist : stmlist . SEMI stm END shift 14 SEMI shift 7 . error state 10: stm : WHILE ID . DO stm DO shift 15 . error state 11: stm : ID ASSIGN . ID ID shift 16 . error state 12: stmlist : stmlist SEMI stm . . reduce by rule 7 state 13: stm : IF ID THEN . stm stm : IF ID THEN . stm ELSE stm ID shift 6 WHILE shift 5 BEGIN shift 4 IF shift 3 stm goto 17 . error state 14: stm : BEGIN stmlist END . . reduce by rule 3 state 15: stm : WHILE ID DO . stm ID shift 6 WHILE shift 5 BEGIN shift 4 IF shift 3 stm goto 18 . error state 16: stm : ID ASSIGN ID . . reduce by rule 1 state 17: shift/reduce con°ict (shift ELSE, reduce 4) stm : IF ID THEN stm . stm : IF ID THEN stm . ELSE stm ELSE shift 19 . reduce by rule 4 state 18: stm : WHILE ID DO stm . . reduce by rule 2 state 19: stm : IF ID THEN stm ELSE . stm ID shift 6 WHILE shift 5 BEGIN shift 4 IF shift 3 stm goto 20 . error state 20: stm : IF ID THEN stm ELSE stm . . reduce by rule 5 state 21: EOF accept . error FIGURE 3.32. LR states for Grammar 3.30. 71 ","CHAPTER THREE. PARSING id num + - * / ( ) $ E 1 s2 s3 s4 g7 2 r1 r1 r1 r1 r1 r1 3 r2 r2 r2 r2 r2 r2 4 s2 s3 s4 g5 5 s6 6 r7 r7 r7 r7 r7 r7 7 s8 s10 s12 s14 a 8 s2 s3 s4 g9 9 s8,r5 s10,r5 s12,r5 s14,r5 r5 r5 10 s2 s3 s4 g11 11 s8,r6 s10,r6 s12,r6 s14,r6 r6 r6 12 s2 s3 s4 g13 13 s8,r3 s10,r3 s12,r3 s14,r3 r3 r3 14 s2 s3 s4 g15 15 s8,r4 s10,r4 s12,r4 s14,r4 r4 r4 TABLE 3.33. LR parsing table for Grammar 3.5. Shift-reduce con°icts are acceptable in a grammar if they correspond to well understood cases, as in this example. But most shift-reduce con°icts, and all reduce-reduce con°icts, are serious problems and should be eliminated by rewriting the grammar. PRECEDENCE DIRECTIVES No ambiguous grammar is LR( k ) for any k ; the LR( k ) parsing table of an am- biguous grammar will always have con°icts. However, ambiguous grammars can still be useful if we can ﬁnd ways to resolve the con°icts. For example, Grammar 3.5 is highly ambiguous. In using this grammar to describe a programming language, we intend it to be parsed so that ∗ and / bind more tightly than + and − , and that each operator associates to the left. We can express this by rewriting the unambiguous Grammar 3.8. But we can avoid introducing the T and F symbols and their associated “trivial” reductions E → T and T → F . Instead, let us start by building the LR(1) parsing table for Grammar 3.5, as shown in Table 3.33 . We ﬁnd many con°icts. For example, in state 13 with lookahead + we ﬁnd a con°ict between shift into state 8 and reduce by rule 3 . Two of the items in state 13 are: 72 ","3.4. USING PARSER GENERATORS E → E ∗ E . + E → E . + E (any) In this state the top of the stack is · · · E ∗ E . Shifting will lead to a stack · · · E ∗ E + and eventually · · · E ∗ E + E with a reduction of E + E to E . Reducing now will lead to the stack · · · E and then the + will be shifted. The parse trees obtained by shifting and reducing are: . E E * E E + E Shift . E E E * E + E Reduce If we wish ∗ to bind tighter than + , we should reduce instead of shift. So we ﬁll the ( 13 , + ) entry in the table with r3 and discard the s8 action. Conversely, in state 9 on lookahead ∗ , we should shift instead of reduce, so we resolve the con°ict by ﬁlling the ( 9 , ∗ ) entry with s12. The case for state 9, lookahead + is E → E + E . + E → E . + E (any) Shifting will make the operator right-associative; reducing will make it left- associative. Since we want left associativity, we ﬁll ( 9 , + ) with r5. Consider the expression a − b − c . In most programming languages, this associates to the left, as if written ( a − b ) − c . But suppose we believe that this expression is inherently confusing, and we want to force the programmer to put in explicit parentheses, either ( a − b ) − c or a − ( b − c ) . Then we say that the minus operator is nonassociative , and we would ﬁll the ( 11 , − ) entry with an error entry. The result of all these decisions is a parsing table with all con°icts resolved (Table 3.34) . Yacc has precedence directives to indicate the resolution of this class of shift-reduce con°icts. A series of declarations such as %nonassoc EQ NEQ %left PLUS MINUS %left TIMES DIV %right EXP 73 ","CHAPTER THREE. PARSING + - * / . . . 9 r5 r5 s12 s14 11 · · · s12 s14 · · · 13 r3 r3 r3 r3 15 r4 r4 . . . TABLE 3.34. Con°icts of Table 3.33 resolved. indicates that + and - are left-associative and bind equally tightly; that * and / are left-associative and bind more tightly than + ; that ˆ is right-associative and binds most tightly; and that = and ̸ = are nonassociative, and bind more weakly than + . In examining a shift-reduce con°ict such as E → E ∗ E . + E → E . + E (any) there is the choice of shifting a token and reducing by a rule . Should the rule or the token be given higher priority? The precedence declarations ( %left , etc.) give priorities to the tokens; the priority of a rule is given by the last token occurring on the right-hand side of that rule. Thus the choice here is between a rule with priority * and a token with priority + ; the rule has higher priority, so the con°ict is resolved in favor of reducing. When the rule and token have equal priority, then a %left precedence favors reducing, %right favors shifting, and %nonassoc yields an error ac- tion. Instead of using the default “rule has precedence of its last token,” we can assign a speciﬁc precedence to a rule using the %prec directive. This is commonly used to solve the “unary minus” problem. In most programming languages a unary minus binds tighter than any binary operator, so − 6 ∗ 8 is parsed as ( − 6 ) ∗ 8, not − ( 6 ∗ 8 ) . Grammar 3.35 shows an example. The token UMINUS is never returned by the lexer; it is merely a place- holder in the chain of precedence ( %left ) declarations. The directive %prec UMINUS gives the rule exp: MINUS exp the highest precedence, so reduc- ing by this rule takes precedence over shifting any operator, even a minus sign. 74 ","3.4. USING PARSER GENERATORS %{ declarations of yylex and yyerror %} %token INT PLUS MINUS TIMES UMINUS %start exp %left PLUS MINUS %left TIMES %left UMINUS %% exp : INT | exp PLUS exp | exp MINUS exp | exp TIMES exp | MINUS exp %prec UMINUS GRAMMAR 3.35. Precedence rules are helpful in resolving con°icts, but they should not be abused. If you have trouble explaining the effect of a clever use of precedence rules, perhaps instead you should rewrite the grammar to be unambiguous. SYNTAX VERSUS SEMANTICS Consider a programming language with arithmetic expressions such as x + y and boolean expressons such as x + y = z or a &amp; ( b = c ) . Arithmetic opera- tors bind tighter than the boolean operators; there are arithmetic variables and boolean variables; and a boolean expression cannot be added to an arithmetic expression. Grammar 3.36 gives a syntax for this language. The grammar has a reduce-reduce con°ict, as shown in Figure 3.37 . How should we rewrite the grammar to eliminate this con°ict? Here the problem is that when the parser sees an identiﬁer such as a , it has no way of knowing whether this is an arithmetic variable or a boolean variable – syntactically they look identical. The solution is to defer this analysis until the “semantic” phase of the compiler; it’s not a problem that can be handled naturally with context-free grammars. A more appropriate grammar is: S → id := E E → id E → E &amp; E E → E = E E → E + E Now the expression a + 5 &amp; b is syntactically legal, and a later phase of the compiler will have to reject it and print a semantic error message. 75 ","CHAPTER THREE. PARSING %{ declarations of yylex and yyerror %} %token ID ASSIGN PLUS MINUS AND EQUAL %start stm %left OR %left AND %left PLUS %% stm : ID ASSIGN ae | ID ASSIGN be be : be OR be | be AND be | ae EQUAL ae | ID ae : ae PLUS ae | ID GRAMMAR 3.36. 3.5 ERROR RECOVERY LR( k ) parsing tables contain shift, reduce, accept, and error actions. On page 59 I claimed that when an LR parser encounters an error action it stops pars- ing and reports failure. This behavior would be unkind to the programmer, who would like to have all the errors in her program reported, not just the ﬁrst error. RECOVERY USING THE ERROR SYMBOL Local error recovery mechanisms work by adjusting the parse stack and the input at the point where the error was detected in a way that will allow parsing to resume. One local recovery mechanism – found in many versions of the Yacc parser generator – uses a special error symbol to control the recovery process. Wherever the special error symbol appears in a grammar rule, a sequence of erroneous input tokens can be matched. For example, in a Yacc grammar for Tiger, we might have productions such as 76 ","3.5. ERROR RECOVERY state 0: stm : . ID ASSIGN ae stm : . ID ASSIGN be ID shift 1 stm goto 14 . error state 1: stm : ID . ASSIGN ae stm : ID . ASSIGN be ASSIGNshift 2 . error state 2: stm : ID ASSIGN . ae stm : ID ASSIGN . be ID shift 5 be goto 4 ae goto 3 . error state 3: stm : ID ASSIGN ae . be : ae . EQUAL ae ae : ae . PLUS ae PLUS shift 7 EQUAL shift 6 . reduce by rule 0 state 4: stm : ID ASSIGN be . be : be . AND be AND shift 8 . reduce by rule 1 state 5: reduce/reduce con°ict between rule 6 and rule 4 on EOF be : ID . ae : ID . PLUS reduce by rule 6 AND reduce by rule 4 EQUAL reduce by rule 6 EOF reduce by rule 4 . error state 6: be : ae EQUAL . ae ID shift 10 ae goto 9 . error state 7: ae : ae PLUS . ae ID shift 10 ae goto 11 . error state 8: be : be AND . be ID shift 5 be goto 13 ae goto 12 . error state 9: be : ae EQUAL ae . ae : ae . PLUS ae PLUS shift 7 . reduce by rule 3 state 10: ae : ID . . reduce by rule 6 state 11: ae : ae . PLUS ae ae : ae PLUS ae . . reduce by rule 5 state 12: be : ae . EQUAL ae ae : ae . PLUS ae PLUS shift 7 EQUAL shift 6 . error state 13: be : be . AND be be : be AND be . . reduce by rule 2 state 14: EOF accept . error FIGURE 3.37. LR states for Grammar 3.36. exp → ID exp → exp + exp exp → ( exps ) exps → exp exps → exps ; exp Informally, we can specify that if a syntax error is encountered in the mid- dle of an expression, the parser should skip to the next semicolon or right parenthesis (these are called syncronizing tokens ) and resume parsing. We do 77 ","CHAPTER THREE. PARSING this by adding error-recovery productions such as exp → ( error ) exps → error ; exp What does the parser-generator do with the error symbol? In parser gener- ation, error is considered a terminal symbol, and shift actions are entered in the parsing table for it as if it were an ordinary token. When the LR parser reaches an error state, it takes the following actions: 1. Pop the stack (if necessary) until a state is reached in which the action for the error token is shift . 2. Shift the error token. 3. Discard input symbols (if necessary) until a state is reached that has a non- error action on the current lookahead token. 4. Resume normal parsing. In the two error productions illustrated above, we have taken care to follow the error symbol with an appropriate synchronizing token – in this case, right parenthesis or semicolon. Thus, the “non-error action” taken in step 3 will always shift . If instead we used the production exp → error , the “non-error action” would be reduce , and (in an SLR or LALR parser) it is possible that the original (erroneous) lookahead symbol would cause another error after the reduce action, without having advanced the input. Therefore, grammar rules that contain error not followed by a token should be used only when there is no good alternative. Caution. One can attach semantic actions to Yacc grammar rules; whenever a rule is reduced, its semantic action is executed. Chapter 4 explains the use of semantic actions. Popping states from the stack can lead to seemingly “im- possible” semantic actions, especially if the actions contain side effects. Con- sider this grammar fragment: statements: statements exp SEMICOLON | statements error SEMICOLON | /* empty */ exp : increment exp decrement | ID increment: LPAREN {nest=nest+1;} decrement: RPAREN {nest=nest-1;} 78 ","3.5. ERROR RECOVERY “Obviously” it is true that whenever a semicolon is reached, the value of nest is zero, because it is incremented and decremented in a balanced way according to the grammar of expressions. But if a syntax error is found after some left parentheses have been parsed, then states will be popped from the stack without “completing” them, leading to a nonzero value of nest . The best solution to this problem is to have side-effect-free semantic actions that build abstract syntax trees, as described in Chapter 4. GLOBAL ERROR REPAIR What if the best way to recover from the error is to insert or delete tokens from the input stream at a point before where the error was detected? Consider the following Tiger program: let type a := intArray [ 10 ] of 0 in . . . A local technique will discover a syntax error with := as lookahead sym- bol. Error recovery based on error productions would likely delete the phrase from type to 0 , resynchronizing on the in token. Some local repair tech- niques can insert tokens as well as delete them; but even a local repair that replaces the := by = is not very good, and will encounter another syntax er- ror at the [ token. Really, the programmer’s mistake here is in using type instead of var , but the error is detected two tokens too late. Global error repair ﬁnds the smallest set of insertions and deletions that would turn the source string into a syntactically correct string, even if the insertions and deletions are not at a point where an LL or LR parser would ﬁrst report an error . In this case, global error repair would do a single-token substitution, replacing type by var . Burke-Fisher error repair. I will describe a limited but useful form of global error repair, which tries every possible single-token insertion, deletion, or replacement at every point that occurs no earlier than K tokens before the point where the parser reported the error. Thus, with K = 15, if the parsing engine gets stuck at the 100th token of the input, then it will try every possible repair between the 85th and 100th token. The correction that allows the parser to parse furthest past the original reported error is taken as the best error repair. Thus, if a single-token substi- tution of var for type at the 98th token allows the parsing engine to proceed past the 104th token without getting stuck, this repair is a successful one. 79 ","CHAPTER THREE. PARSING a := 7 Old num 10 Stack := 6 id 4 ↓ ; b := c + ( ± ²³ ´ 6-token queue Current ( 8 Stack + 16 E 11 := 6 id 4 ; 3 S 2 ↓ d := 5 + 6 , d ) $ FIGURE 3.38. Burke-Fisher parsing, with an error-repair queue. Figure 3.18 shows the complete parse of this string according to Table 3.19 . Generally, if a repair carries the parser R = 4 tokens beyond where it origi- nally got stuck, this is “good enough.” The advantage of this technique is that the LL( k ) or LR( k ) (or LALR, etc.) grammar is not modiﬁed at all (no error productions), nor are the parsing tables modiﬁed. Only the parsing engine, which interprets the parsing tables, is modiﬁed. The parsing engine must be able to back up K tokens and reparse. To do this, it needs to remember what the parse stack looked like K tokens ago. Therefore, the algorithm maintains two parse stacks: the current stack and the old stack. A queue of K tokens is kept; as each new token is shifted, it is pushed on the current stack and also put onto the tail of the queue; simul- taneously, the head of the queue is removed and shifted onto the old stack. With each shift onto the old or current stack, the appropriate reduce actions are also performed. Figure 3.38 illustrates the two stacks and queue. Now suppose a syntax error is detected at the current token. For each possi- ble insertion, deletion, or substitution of a token at any position of the queue, the Burke-Fisher error repairer makes that change to within (a copy of) the queue, then attempts to reparse from the old stack. The success of a modiﬁ- cation is in how many tokens past the current token can be parsed; generally, if three or four new tokens can be parsed, this is considered a completely successful repair. In a language with N kinds of tokens, there are K + K · N + K · N possible deletions, insertions, and substitutions within the K -token window. Trying 80 ","3.5. ERROR RECOVERY this many repairs is not very costly, especially considering that it happens only when a syntax error is discovered, not during ordinary parsing. Semantic actions. Shift and reduce actions are tried repeatly and discarded during the search for the best error repair. Parser generators usually perform programmer-speciﬁed semantic actions along with each reduce action, but the programmer does not expect that these actions will be performed repeatedly and discarded – they may have side effects. Therefore, a Burke-Fisher parser does not execute any of the semantic actions as reductions are performed on the current stack, but waits until the same reductions are performed (perma- nently) on the old stack. This means that the lexical analyzer may be up to K + R tokens ahead of the point to which semantic actions have been performed. If semantic actions affect lexical analysis – as they do in C, compiling the typedef feature – this can be a problem with the Burke-Fisher approach. For languages with a pure context-free grammar approach to syntax, the delay of semantic actions poses no problem. Semantic values for insertions. In repairing an error by insertion, the parser needs to provide a semantic value for each token it inserts, so that semantic actions can be performed as if the token had come from the lexical analyzer. For punctuation tokens no value is necessary, but when tokens such as num- bers or identiﬁers must be inserted, where can the value come from? The ML-Yacc parser generator, which uses Burke-Fischer error correction, has a %value directive, allowing the programmer to specify what value should be used when inserting each kind of token: %value ID (bogus) %value INT (1) %value STRING () Programmer-speciﬁed substitutions. Some common kinds of errors cannot be repaired by the insertion or deletion of a single token, and sometimes a particular single-token insertion or substitution is very commonly required and should be tried ﬁrst. Therefore, in an ML-Yacc grammar speciﬁcation the programmer can use the %change directive to suggest error corrections to be tried ﬁrst, before the default “delete or insert each possible token” repairs. %change EQ -&gt; ASSIGN | ASSIGN -&gt; EQ | SEMICOLON ELSE -&gt; ELSE | -&gt; IN INT END 81 ","CHAPTER THREE. PARSING Here the programmer is suggesting that users often write “ ; else ” where they mean “ else ” and so on. The insertion of in 0 end is a particularly important kind of correction, known as a scope closer . Programs commonly have extra left parentheses or right parentheses, or extra left or right brackets, and so on. In Tiger, another kind of nesting construct is let · · · in · · · end . If the programmer forgets to close a scope that was opened by left parenthesis, then the automatic single- token insertion heuristic can close this scope where necessary. But to close a let scope requires the insertion of three tokens, which will not be done automatically unless the compiler-writer has suggested “change nothing to in 0 end ” as illustrated in the %change command above. P R O G R A M PARSING Use Yacc to implement a parser for the Tiger language. Appendix A de- scribes, among other things, the syntax of Tiger. You should turn in the ﬁle tiger.grm and a README . Supporting ﬁles available in $TIGER/chap3 include: makefile The “makeﬁle.” errormsg.[ch] The Error Message structure, useful for producing error mes- sages with ﬁle names and line numbers. lex.yy.c The lexical analyzer. I haven’t provided the source ﬁle tiger.lex , but I’ve provided the output of Lex that you can use if your lexer isn’t working. parsetest.c A driver to run your parser on an input ﬁle. tiger.grm The skeleton of a ﬁle you must ﬁll in. You won’t need tokens.h anymore; instead, the header ﬁle for tokens is y.tab.h , which is produced automatically by Yacc from the token speciﬁ- cation of your grammar. Your grammar should have as few shift-reduce con°icts as possible, and no reduce-reduce con°icts. Furthermore, your accompanying documentation should list each shift-reduce con°ict (if any) and explain why it is not harmful. My grammar has a shift-reduce con°ict that’s related to the confusion be- tween variable [ expression ] type-id [ expression ] of expression In fact, I had to add a seemingly redundant grammar rule to handle this con- fusion. Is there a way to do this without a shift-reduce con°ict? 82 "]