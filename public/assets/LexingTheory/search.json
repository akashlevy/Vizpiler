["6 Lex Theory During the first phase the compiler reads the input and converts strings in the source to tokens. With regular expressions we can specify patterns to lex so it can generate code that will allow it to scan and match strings in the input. Each pattern specified in the input to lex has an associated action. Typically an action returns a token that represents the matched string for subsequent use by the parser. Initially we will simply print the matched string rather than return a token value. The following represents a simple pattern, composed of a regular expression, that scans for identifiers. Lex will read this pattern and produce C code for a lexical analyzer that scans for identifiers. letter(letter|digit)* This pattern matches a string of characters that begins with a single letter followed by zero or more letters or digits. This example nicely illustrates operations allowed in regular expressions: • repetition, expressed by the “ * ” operator • alternation, expressed by the “ | ” operator • concatenation Any regular expression expressions may be expressed as a finite state automaton (FSA). We can represent an FSA using states, and transitions between states. There is one start state and one or more final or accepting states. Figure 3 : Finite State Automaton In Figure 3 state 0 is the start state and state 2 is the accepting state. As characters are read we make a transition from one state to another. When the first letter is read we transition to state 1. We remain in state 1 as more letters or digits are read. When we read a character other than a letter or digit we transition to accepting state 2. Any FSA may be expressed as a computer program. For example, our 3-state machine is easily programmed: start: goto state0 state0: read c if c = letter goto state1 goto state0 state1: read c if c = letter goto state1 if c = digit goto state1 goto state2 state2: accept string 0 1 2 letter letter or digit other start ","7 This is the technique used by lex. Regular expressions are translated by lex to a computer program that mimics an FSA. Using the next input character and current state the next state is easily determined by indexing into a computer-generated state table. Now we can easily understand some of lex’s limitations. For example, lex cannot be used to recognize nested structures such as parentheses. Nested structures are handled by incorporating a stack. Whenever we encounter a “ ( ” we push it on the stack. When a “ ) ” is encountered we match it with the top of the stack and pop the stack. However lex only has states and transitions between states. Since it has no stack it is not well suited for parsing nested structures. Yacc augments an FSA with a stack and can process constructs such as parentheses with ease. The important thing is to use the right tool for the job. Lex is good at pattern matching. Yacc is appropriate for more challenging tasks. Practice Metacharacter Matches . any character except newline n newline * zero or more copies of the preceding expression + one or more copies of the preceding expression ? zero or one copy of the preceding expression ^ beginning of line $ end of line a|b a or b (ab)+ one or more copies of ab (grouping) a+b literal  a+b  (C escapes still work) [] character class Table 1 : Pattern Matching Primitives Expression Matches abc abc abc* ab abc abcc abccc ... abc+ abc abcc abccc ... a(bc)+ abc abcbc abcbcbc ... a(bc)? a abc [abc] one of: a , b , c [a-z] any letter, a-z [a-z] one of: a , - , z [-az] one of: - , a , z [A-Za-z0-9]+ one or more alphanumeric characters [ tn]+ whitespace [^ab] anything except: a , b [a^b] one of: a , ^ , b [a|b] one of: a , | , b a|b one of: a , b Table 2 : Pattern Matching Examples Regular expressions in lex are composed of metacharacters (Table 1). Pattern-matching examples are shown in Table 2. Within a character class normal operators lose their meaning. Two operators ","8 allowed in a character class are the hyphen (“ - ”) and circumflex (“ ^ ”). When used between two characters the hyphen represents a range of characters. The circumflex, when used as the first character, negates the expression. If two patterns match the same string, the longest match wins. In case both matches are the same length, then the first pattern listed is used. ... definitions ... %% ... rules ... %% ... subroutines ... Input to Lex is divided into three sections with %% dividing the sections. This is best illustrated by example. The first example is the shortest possible lex file: %% Input is copied to output one character at a time. The first %% is always required, as there must always be a rules section. However if we don’t specify any rules then the default action is to match everything and copy it to output. Defaults for input and output are stdin and stdout , respectively. Here is the same example with defaults explicitly coded: %% /* match everything except newline */ . ECHO; /* match newline */ n ECHO; %% int yywrap(void) { return 1; } int main(void) { yylex(); return 0; } Two patterns have been specified in the rules section. Each pattern must begin in column one. This is followed by whitespace (space, tab or newline) and an optional action associated with the pattern. The action may be a single C statement, or multiple C statements, enclosed in braces. Anything not starting in column one is copied verbatim to the generated C file. We may take advantage of this behavior to specify comments in our lex file. In this example there are two patterns, “ . ” and “ n ”, with an ECHO action associated for each pattern. Several macros and variables are predefined by lex. ECHO is a macro that writes code matched by the pattern. This is the default action for any unmatched strings. Typically, ECHO is defined as: #define ECHO fwrite(yytext, yyleng, 1, yyout) Variable yytext is a pointer to the matched string (NULL-terminated) and yyleng is the length of the matched string. Variable yyout is the output file and defaults to stdout. Function yywrap is called by lex when input is exhausted. Return 1 if you are done or 0 if more processing is required. Every C program requires a main function. In this case we simply call yylex that is the main entry- point for lex. Some implementations of lex include copies of main and yywrap in a library thus eliminating the need to code them explicitly. This is why our first example, the shortest lex program, functioned properly. ","9 Name Function int yylex(void) call to invoke lexer, returns token char *yytext pointer to matched string yyleng length of matched string yylval value associated with token int yywrap(void) wrapup, return 1 if done, 0 if not done FILE *yyout output file FILE *yyin input file INITIAL initial start condition BEGIN condition switch start condition ECHO write matched string Table 3 : Lex Predefined Variables Here is a program that does nothing at all. All input is matched but no action is associated with any pattern so there will be no output. %% . n The following example prepends line numbers to each line in a file. Some implementations of lex predefine and calculate yylineno . The input file for lex is yyin and defaults to stdin . %{ int yylineno; %} %% ^(.*)n printf(%4dt%s, ++yylineno, yytext); %% int main(int argc, char *argv[]) { yyin = fopen(argv[1], r); yylex(); fclose(yyin); } ","10 The definitions section is composed of substitutions, code, and start states. Code in the definitions section is simply copied as-is to the top of the generated C file and must be bracketed with “ %{ “ and “ %} ” markers. Substitutions simplify pattern-matching rules. For example, we may define digits and letters: digit  [0-9] letter [A-Za-z] %{ int count; %} %% /* match identifier */ {letter}({letter}|{digit})* count++; %% int main(void) { yylex(); printf(number of identifiers = %dn, count); return 0; } Whitespace must separate the defining term and the associated expression. References to substitutions in the rules section are surrounded by braces ( {letter} ) to distinguish them from literals. When we have a match in the rules section the associated C code is executed. Here is a scanner that counts the number of characters, words, and lines in a file (similar to Unix wc): %{ int nchar, nword, nline; %} %% n { nline++; nchar++; } [^ tn]+ { nword++, nchar += yyleng; } . { nchar++; } %% int main(void) { yylex(); printf(%dt%dt%dn, nchar, nword, nline); return 0; } ","18 Calculator Description This version of the calculator is substantially more complex than previous versions. Major changes include control constructs such as if-else and while . In addition a syntax tree is constructed during parsing. After parsing we walk the syntax tree to produce output. Three versions of the tree walk routine are supplied: • an interpreter that executes statements during the tree walk • a compiler that generates code for a hypothetical stack-based machine • a version that generates a syntax tree of the original program To make things more concrete, here is a sample program, x = 0; while (x &lt; 3) { print x; x = x + 1; } with output for the interpretive version, 0 1 2 an output for the compiler version, push  0 pop   x L000: push  x push  3 compLT jz   L001 push  x print push x push 1 add pop x jmp L000 L001: ","19 and a version that generates a syntax tree. Graph 0: [=] | |----| | | id(X) c(0) Graph 1: while | |----------------| | | [&lt;] [;] | | |----| |----------| | | | | id(X) c(3) print [=] | | | |-------| | | | id(X) id(X) [+] | |----| | | id(X) c(1) The include file contains declarations for the syntax tree and symbol table. Symbol table ( sym ) allows for single-character variable names. A node in the syntax tree may hold a constant ( conNodeType ), an identifier ( idNodeType ), or an internal node with an operator ( oprNodeType ). A union encapsulates all three variants and nodeType.type is used to determine which structure we have. The lex input file contains patterns for VARIABLE and INTEGER tokens. In addition, tokens are defined for 2-character operators such as EQ and NE . Single-character operators are simply returned as themselves. The yacc input file defines YYSTYPE , the type of yylval , as %union { int iValue;     /* integer value */ char sIndex;     /* symbol table index */ nodeType *nPtr;    /* node pointer */ }; This causes the following to be generated in y.tab.h: typedef union { int iValue;     /* integer value */ char sIndex;     /* symbol table index */ nodeType *nPtr;    /* node pointer */ } YYSTYPE; extern YYSTYPE yylval; ","20 Constants, variables, and nodes can be represented by yylval in the parser’s value stack. A more accurate representation of decimal integers is given below. This is similar to C/C++ where integers that begin with 0 are classified as octal. 0 { yylval.iValue = atoi(yytext); return INTEGER; } [1-9][0-9]* { yylval.iValue = atoi(yytext); return INTEGER; } Notice the type definitions %token &lt;iValue&gt; INTEGER %type &lt;nPtr&gt; expr This binds expr to nPtr , and INTEGER to iValue in the YYSTYPE union. This is required so that yacc can generate the correct code. For example, the rule expr: INTEGER { $$ = con($1); } should generate the following code. Note that yyvsp[0] addresses the top of the value stack, or the value associated with INTEGER . yylval.nPtr = con(yyvsp[0].iValue); The unary minus operator is given higher priority than binary operators as follows: %left GE LE EQ NE '&gt;' '&lt;' %left '+' '-' %left '*' '/' %nonassoc UMINUS The %nonassoc indicates no associativity is implied. It is frequently used in conjunction with %prec to specify precedence of a rule. Thus, we have expr: '-' expr %prec UMINUS { $$ = node(UMINUS, 1, $2); } indicating that the precedence of the rule is the same as the precedence of token UMINUS . And UMINUS (as defined above) has higher precedence than the other operators. A similar technique is used to remove ambiguity associated with the if-else statement (see If-Else Ambiguity). The syntax tree is constructed bottom-up allocating the leaf nodes when variables and integers are reduced. When operators are encountered a node is allocated and pointers to previously allocated nodes are entered as operands. After the tree is built function ex is called to do a depth-first walk of the syntax tree. A depth-first walk visits nodes in the order that they were originally allocated. This results in operators being applied in the order that they were encountered during parsing. Three versions of ex are included: an interpretive version, a compiler version, and a version that generates a syntax tree. ","22 Lex Input %{ #include &lt;stdlib.h&gt; #include calc3.h #include y.tab.h void yyerror(char *); %} %% [a-z] { yylval.sIndex = *yytext - 'a'; return VARIABLE; } 0 { yylval.iValue = atoi(yytext); return INTEGER; } [1-9][0-9]* { yylval.iValue = atoi(yytext); return INTEGER; } [-()&lt;&gt;=+*/;{}.] { return *yytext; } &gt;= return GE; &lt;= return LE; == return EQ; != return NE; while return WHILE; if return IF; else return ELSE; print return PRINT; [ tn]+ ; /* ignore whitespace */ . yyerror(Unknown character); %% int yywrap(void) { return 1; } ","34 More Lex Strings Quoted strings frequently appear in programming languages. Here is one way to match a string in lex: %{ char *yylval; #include &lt;string.h&gt; %} %% [^n]*[n] { yylval = strdup(yytext+1); if (yylval[yyleng-2] != '') warning(improperly terminated string); else yylval[yyleng-2] = 0; printf(found '%s'n, yylval); } The above example ensures that strings don’t cross line boundaries and removes enclosing quotes. If we wish to add escape sequences, such as n or  , start states simplify matters: %{ char buf[100]; char *s; %} %x STRING %%  { BEGIN STRING; s = buf; } &lt;STRING&gt;n { *s++ = 'n'; } &lt;STRING&gt;t { *s++ = 't'; } &lt;STRING&gt; { *s++ = ''; } &lt;STRING&gt; { *s = 0; BEGIN 0; printf(found '%s'n, buf); } &lt;STRING&gt;n { printf(invalid string); exit(1); } &lt;STRING&gt;. { *s++ = *yytext; } Exclusive start state STRING is defined in the definition section. When the scanner detects a quote the BEGIN macro shifts lex into the STRING state. Lex stays in the STRING state and recognizes only patterns that begin with &lt;STRING&gt; until another BEGIN is executed. Thus we have a mini- environment for scanning strings. When the trailing quote is recognized we switch back to initial state 0. Reserved Words If your program has a large collection of reserved words it is more efficient to let lex simply match a string and determine in your own code whether it is a variable or reserved word. For example, instead of coding ","35 if return IF; then return THEN; else return ELSE; {letter}({letter}|{digit})* { yylval.id = symLookup(yytext); return IDENTIFIER; } where symLookup returns an index into the symbol table, it is better to detect reserved words and identifiers simultaneously, as follows: {letter}({letter}|{digit})* { int i; if ((i = resWord(yytext)) != 0) return (i); yylval.id = symLookup(yytext); return (IDENTIFIER); } This technique significantly reduces the number of states required, and results in smaller scanner tables. Debugging Lex Lex has facilities that enable debugging. This feature may vary with different versions of lex so you should consult documentation for details. The code generated by lex in file lex.yy.c includes debugging statements that are enabled by specifying command-line option “ -d ”. Debug output in flex (a GNU version of lex) may be toggled on and off by setting yy_flex_debug . Output includes the rule applied and corresponding matched text. If you’re running lex and yacc together then specify the following in your yacc input file: extern int yy_flex_debug; int main(void) { yy_flex_debug = 1; yyparse(); } Alternatively, you may write your own debug code by defining functions that display information for the token value and each variant of the yylval union. This is illustrated in the following example. When DEBUG is defined the debug functions take effect and a trace of tokens and associated values is displayed. %union { int ivalue; ... }; %{ #ifdef DEBUG int dbgToken(int tok, char *s) { printf(token %sn, s); return tok; } int dbgTokenIvalue(int tok, char *s) { ","36 printf(token %s (%d)n, s, yylval.ivalue); return tok; } #define RETURN(x) return dbgToken(x, #x) #define RETURN_ivalue(x) return dbgTokenIvalue(x, #x) #else #define RETURN(x) return(x) #define RETURN_ivalue(x) return(x) #endif %} %% [0-9]+ { yylval.ivalue = atoi(yytext); RETURN_ivalue(INTEGER); } if RETURN(IF); else RETURN(ELSE); More Yacc Recursion A list may be specified with left recursion list: item | list ',' item ; or right recursion. list: item | item ',' list If right recursion is used then all items on the list are pushed on the stack. After the last item is pushed we start reducing. With left recursion we never have more than three terms on the stack since we reduce as we go along. For this reason it is advantageous to use left recursion. ","2 Lexical Analysis lex-i-cal : of or relating to words or the vocabulary of a language as distinguished from its grammar and con- struction Webster’s Dictionary To translate a program from one language into another, a compiler must ﬁrst pull it apart and understand its structure and meaning, then put it together in a different way. The front end of the compiler performs analysis; the back end does synthesis. The analysis is usually broken up into Lexical analysis: breaking the input into individual words or “tokens”; Syntax analysis: parsing the phrase structure of the program; and Semantic analysis: calculating the program’s meaning. The lexical analyzer takes a stream of characters and produces a stream of names, keywords, and punctuation marks; it discards white space and com- ments between the tokens. It would unduly complicate the parser to have to account for possible white space and comments at every possible point; this is the main reason for separating lexical analysis from parsing. Lexical analysis is not very complicated, but we will attack it with high- powered formalisms and tools, because similar formalisms will be useful in the study of parsing and similar tools have many applications in areas other than compilation. 16 ","2.1. LEXICAL TOKENS 2.1 LEXICAL TOKENS A lexical token is a sequence of characters that can be treated as a unit in the grammar of a programming language. A programming language classiﬁes lexical tokens into a ﬁnite set of token types. For example, some of the token types of a typical programming language are: Type Examples ID foo n14 last NUM 73 0 00 515 082 REAL 66.1 .5 10. 1e67 5.5e-10 IF if COMMA , NOTEQ != LPAREN ( RPAREN ) Punctuation tokens such as IF , VOID , RETURN constructed from alphabetic characters are called reserved words and, in most languages, cannot be used as identiﬁers. Examples of nontokens are comment /* try again */ preprocessor directive #include&lt;stdio.h&gt; preprocessor directive #define NUMS 5 , 6 macro NUMS blanks, tabs, and newlines In languages weak enough to require a macro preprocessor, the prepro- cessor operates on the source character stream, producing another character stream that is then fed to the lexical analyzer. It is also possible to integrate macro processing with lexical analysis. Given a program such as float match0(char *s) /* find a zero */ {if (!strncmp(s, 0.0, 3)) return 0.; } the lexical analyzer will return the stream FLOAT ID ( match0 ) LPAREN CHAR STAR ID ( s ) RPAREN LBRACE IF LPAREN BANG ID ( strncmp ) LPAREN ID ( s ) 17 ","CHAPTER TWO. LEXICAL ANALYSIS COMMA STRING ( 0.0 ) COMMA NUM ( 3 ) RPAREN RPAREN RETURN REAL ( 0.0 ) SEMI RBRACE EOF where the token-type of each token is reported; some of the tokens, such as identiﬁers and literals, have semantic values attached to them, giving auxil- iary information in addition to the token type. How should the lexical rules of a programming language be described? In what language should a lexical analyzer be written? We can describe the lexical tokens of a language in English; here is a de- scription of identiﬁers in C or Java: An identiﬁer is a sequence of letters and digits; the ﬁrst character must be a letter. The underscore _ counts as a letter. Upper- and lowercase letters are different. If the input stream has been parsed into tokens up to a given char- acter, the next token is taken to include the longest string of characters that could possibly constitute a token. Blanks, tabs, newlines, and comments are ignored except as they serve to separate tokens. Some white space is required to separate otherwise adjacent identiﬁers, keywords, and constants. And any reasonable programming language serves to implement an ad hoc lexer. But we will specify lexical tokens using the formal language of regular expressions , implement lexers using deterministic ﬁnite automata , and use mathematics to connect the two. This will lead to simpler and more readable lexical analyzers. 2.2 REGULAR EXPRESSIONS Let us say that a language is a set of strings ; a string is a ﬁnite sequence of symbols . The symbols themselves are taken from a ﬁnite alphabet . The Pascal language is the set of all strings that constitute legal Pascal programs; the language of primes is the set of all decimal-digit strings that represent prime numbers; and the language of C reserved words is the set of all alphabetic strings that cannot be used as identiﬁers in the C programming language. The ﬁrst two of these languages are inﬁnite sets; the last is a ﬁnite set. In all of these cases, the alphabet is the ASCII character set. When we speak of languages in this way, we will not assign any meaning to the strings; we will just be attempting to classify each string as in the language or not. To specify some of these (possibly inﬁnite) languages with ﬁnite descrip- 18 ","2.2. REGULAR EXPRESSIONS tions, we will use the notation of regular expressions . Each regular expression stands for a set of strings. Symbol: For each symbol a in the alphabet of the language, the regular expres- sion a denotes the language containing just the string a . Alternation: Given two regular expressions M and N , the alternation operator written as a vertical bar | makes a new regular expression M | N . A string is in the language of M | N if it is in the language of M or in the language of N . Thus, the language of a | b contains the two strings a and b . Concatenation: Given two regular expressions M and N , the concatenation operator · makes a new regular expression M · N . A string is in the language of M · N if it is the concatenation of any two strings α and β such that α is in the language of M and β is in the language of N . Thus, the regular expression ( a | b ) · a deﬁnes the language containing the two strings aa and ba . Epsilon: The regular expression ϵ represents a language whose only string is the empty string. Thus, ( a · b ) | ϵ represents the language {  , ab } . Repetition: Given a regular expression M , its Kleene closure is M ∗ . A string is in M ∗ if it is the concatenation of zero or more strings, all of which are in M . Thus, (( a | b ) · a ) ∗ represents the inﬁnite set {  , aa , ba , aaaa , baaa , aaba , baba , aaaaaa , . . . } . Using symbols, alternation, concatenation, epsilon, and Kleene closure we can specify the set of ASCII characters corresponding to the lexical tokens of a programming language. First, consider some examples: ( 0 | 1 ) ∗ · 0 Binary numbers that are multiples of two. b ∗ ( abb ∗ ) ∗ ( a | ϵ ) Strings of a ’s and b ’s with no consecutive a ’s. ( a | b ) ∗ aa ( a | b ) ∗ Strings of a ’s and b ’s containing consecutive a ’s. In writing regular expressions, we will sometimes omit the concatenation symbol or the epsilon, and we will assume that Kleene closure “binds tighter” than concatenation, and concatenation binds tighter than alternation; so that ab | c means ( a · b ) | c , and ( a | ) means ( a | ϵ ) . Let us introduce some more abbreviations: [ abcd ] means ( a | b | c | d ) , [ b - g ] means [ bcdefg ] , [ b - gM - Qkr ] means [ bcdefgMNOPQkr ] , M ? means ( M | ϵ ) , and M + means ( M · M ∗ ) . These extensions are convenient, but none extend the descriptive power of regular expressions: Any set of strings that can be described with these abbreviations could also be described by just the basic set of operators. All the operators are summarized in Figure 2.1 . Using this language, we can specify the lexical tokens of a programming language ( Figure 2.2 ). For each token, we supply a fragment of C code that reports which token type has been recognized. 19 ","CHAPTER TWO. LEXICAL ANALYSIS a An ordinary character stands for itself. ϵ The empty string. Another way to write the empty string. M | N Alternation, choosing from M or N . M · N Concatenation, an M followed by an N . MN Another way to write concatenation. M ∗ Repetition (zero or more times). M + Repetition, one or more times. M ? Optional, zero or one occurrence of M . [ a − zA − Z ] Character set alternation. . A period stands for any single character except newline. a.+* Quotation, a string in quotes stands for itself literally. FIGURE 2.1. Regular expression notation. if {return IF;} [a-z][a-z0-9]* {return ID;} [0-9]+ {return NUM;} ([0-9]+.[0-9]*)|([0-9]*.[0-9]+) {return REAL;} (--[a-z]*n)|( |n|t)+ { /* do nothing */ } . {error();} FIGURE 2.2. Regular expressions for some tokens. The ﬁfth line of the description recognizes comments or white space, but does not report back to the parser. Instead, the white space is discarded and the lexer resumed. The comments for this lexer begin with two dashes, contain only alphabetic characters, and end with newline. Finally, a lexical speciﬁcation should be complete , always matching some initial substring of the input; we can always achieve this by having a rule that matches any single character (and in this case, prints an “illegal character” error message and continues). These rules are a bit ambiguous. For example, does if8 match as a single identiﬁer or as the two tokens if and 8 ? Does the string if 89 begin with an identiﬁer or a reserved word? There are two important disambiguation rules used by Lex and other similar lexical-analyzer generators: Longest match: The longest initial substring of the input that can match any regular expression is taken as the next token. Rule priority: For a particular longest initial substring, the ﬁrst regular expres- 20 ","2.3. FINITE AUTOMATA i 1 3 2 f 2 1 a-z a-z 0-9 2 1 0-9 0-9 IF ID NUM 0-9 0-9 0-9 5 1 3 4 0-9 2 . . 0-9 n 5 1 blank, etc. 2 a-z 3 4 blank, etc. - - any but n 1 2 REAL white space error FIGURE 2.3. Finite automata for lexical tokens. The states are indicated by circles; ﬁnal states are indicated by double circles. The start state has an arrow coming in from nowhere. An edge labeled with several characters is shorthand for many parallel edges. sion that can match determines its token type. This means that the order of writing down the regular-expression rules has signiﬁcance. Thus, if8 matches as an identiﬁer by the longest-match rule, and if matches as a reserved word by rule-priority. 2.3 FINITE AUTOMATA Regular expressions are convenient for specifying lexical tokens, but we need a formalism that can be implemented as a computer program. For this we can use ﬁnite automata (N.B. the singular of automata is automaton). A ﬁnite automaton has a ﬁnite set of states ; edges lead from one state to another, and each edge is labeled with a symbol . One state is the start state, and certain of the states are distinguished as ﬁnal states. Figure 2.3 shows some ﬁnite automata. We number the states just for con- venience in discussion. The start state is numbered 1 in each case. An edge labeled with several characters is shorthand for many parallel edges; so in the ID machine there are really 26 edges each leading from state 1 to 2, each labeled by a different letter. 21 ","CHAPTER TWO. LEXICAL ANALYSIS n 1 a-z 2 - 3 4 13 12 9 11 6 7 8 10 5 i 0-9 0-9 . 0-9 f 0-9 0-9 - 0-9 , a-z a-h j-z other blank, etc. blank, etc. ID IF REAL ID NUM REAL error white space white space error error a-z 0-9 a-e , g-z , 0-9 . FIGURE 2.4. Combined ﬁnite automaton. In a deterministic ﬁnite automaton (DFA), no two edges leaving from the same state are labeled with the same symbol. A DFA accepts or rejects a string as follows. Starting in the start state, for each character in the input string the automaton follows exactly one edge to get to the next state. The edge must be labeled with the input character. After making n transitions for an n -character string, if the automaton is in a ﬁnal state, then it accepts the string. If it is not in a ﬁnal state, or if at some point there was no appropriately labeled edge to follow, it rejects. The language recognized by an automaton is the set of strings that it accepts. For example, it is clear that any string in the language recognized by au- tomaton ID must begin with a letter. Any single letter leads to state 2, which is ﬁnal; so a single-letter string is accepted. From state 2, any letter or digit leads back to state 2, so a letter followed by any number of letters and digits is also accepted. In fact, the machines shown in Figure 2.3 accept the same languages as the regular expressions of Figure 2.2 . These are six separate automata; how can they be combined into a single machine that can serve as a lexical analyzer? We will study formal ways of doing this in the next section, but here we will just do it ad hoc: Figure 2.4 shows such a machine. Each ﬁnal state must be labeled with the token-type 22 ","2.3. FINITE AUTOMATA that it accepts. State 2 in this machine has aspects of state 2 of the IF machine and state 2 of the ID machine; since the latter is ﬁnal, then the combined state must be ﬁnal. State 3 is like state 3 of the IF machine and state 2 of the ID machine; because these are both ﬁnal we use rule priority to disambiguate – we label state 3 with IF because we want this token to be recognized as a reserved word, not an identiﬁer. We can encode this machine as a transition matrix: a two-dimensional ar- ray (a vector of vectors), subscripted by state number and input character. There will be a “dead” state (state 0) that loops to itself on all characters; we use this to encode the absence of an edge. int edges[][256]={ /* · · · 0 1 2 · · · - · · · e f g h i j · · · */ /* state 0 */ {0,0, ·· · 0,0,0 ·· · 0 · · · 0,0,0,0,0,0 ·· · }, /* state 1 */ {0,0, ·· · 7,7,7 ·· · 9 · · · 4,4,4,4,2,4 ·· · }, /* state 2 */ {0,0, ·· · 4,4,4 ·· · 0 · · · 4,3,4,4,4,4 ·· · }, /* state 3 */ {0,0, ·· · 4,4,4 ·· · 0 · · · 4,4,4,4,4,4 ·· · }, /* state 4 */ {0,0, ·· · 4,4,4 ·· · 0 · · · 4,4,4,4,4,4 ·· · }, /* state 5 */ {0,0, ·· · 6,6,6 ·· · 0 · · · 0,0,0,0,0,0 ·· · }, /* state 6 */ {0,0, ·· · 6,6,6 ·· · 0 · · · 0,0,0,0,0,0 ·· · }, /* state 7 */ {0,0, ·· · 7,7,7 ·· · 0 · · · 0,0,0,0,0,0 ·· · }, /* state 8 */ {0,0, ·· · 8,8,8 ·· · 0 · · · 0,0,0,0,0,0 ·· · }, et cetera } There must also be a “ﬁnality” array, mapping state numbers to actions – ﬁnal state 2 maps to action ID , and so on. RECOGNIZING THE LONGEST MATCH It is easy to see how to use this table to recognize whether to accept or reject a string, but the job of a lexical analyzer is to ﬁnd the longest match, the longest initial substring of the input that is a valid token. While interpreting transitions, the lexer must keep track of the longest match seen so far, and the position of that match. Keeping track of the longest match just means remembering the last time the automaton was in a ﬁnal state with two variables, Last-Final (the state number of the most recent ﬁnal state encountered) and Input-Position- at-Last-Final . Every time a ﬁnal state is entered, the lexer updates these variables; when a dead state (a nonﬁnal state with no output transitions) is reached, the variables tell what token was matched, and where it ended. Figure 2.5 shows the operation of a lexical analyzer that recognizes longest matches; note that the current input position may be far beyond the most recent position at which the recognizer was in a ﬁnal state. 23 ","CHAPTER TWO. LEXICAL ANALYSIS Last Current Current Accept Final State Input Action 0 1 | ⊤ ⊥ if --not-a-com 2 2 | i ⊤ ⊥ f --not-a-com 3 3 | if ⊤ ⊥ --not-a-com 3 0 | if ⊤ ⊥ --not-a-com return IF 0 1 if | ⊤ ⊥ --not-a-com 12 12 if | ⊤ ⊥ --not-a-com 12 0 if | ⊤ - ⊥ -not-a-com found white space; resume 0 1 if | ⊤ ⊥ --not-a-com 9 9 if | - ⊤ ⊥ -not-a-com 9 10 if | - ⊤ - ⊥ not-a-com 9 10 if | - ⊤ -n ⊥ ot-a-com 9 10 if | - ⊤ -no ⊥ t-a-com 9 10 if | - ⊤ -not ⊥ -a-com 9 0 if | - ⊤ -not- ⊥ a-com error, illegal token ‘ - ’; resume 0 1 if - | ⊤ ⊥ -not-a-com 9 9 if - | - ⊤ ⊥ not-a-com 9 0 if - | - ⊤ n ⊥ ot-a-com error, illegal token ‘ - ’; resume FIGURE 2.5. The automaton of Figure 2.4 recognizes several tokens. The symbol | indicates the input position at each successive call to the lexical analyzer, the symbol ⊥ indicates the current position of the automaton, and ⊤ indicates the most recent position in which the recognizer was in a ﬁnal state. 2.4 NONDETERMINISTIC FINITE AUTOMATA A nondeterministic ﬁnite automaton (NFA) is one that has a choice of edges – labeled with the same symbol – to follow out of a state. Or it may have special edges labeled with ϵ (the Greek letter epsilon), that can be followed without eating any symbol from the input. Here is an example of an NFA: a a a a a a a 24 ","2.4. NONDETERMINISTIC FINITE AUTOMATA In the start state, on input character a , the automaton can move either right or left. If left is chosen, then strings of a ’s whose length is a multiple of three will be accepted. If right is chosen, then even-length strings will be accepted. Thus, the language recognized by this NFA is the set of all strings of a ’s whose length is a multiple of two or three. On the ﬁrst transition, this machine must choose which way to go. It is required to accept the string if there is any choice of paths that will lead to acceptance. Thus, it must “guess,” and must always guess correctly. Edges labeled with ϵ may be taken without using up a symbol from the input. Here is another NFA that accepts the same language: a a a a a ∋ ∋ Again, the machine must choose which ϵ -edge to take. If there is a state with some ϵ -edges and some edges labeled by symbols, the machine can choose to eat an input symbol (and follow the corresponding symbol-labeled edge), or to follow an ϵ -edge instead. CONVERTING A REGULAR EXPRESSION TO AN NFA Nondeterministic automata are a useful notion because it is easy to convert a (static, declarative) regular expression to a (simulatable, quasi-executable) NFA. The conversion algorithm turns each regular expression into an NFA with a tail (start edge) and a head (ending state). For example, the single-symbol regular expression a converts to the NFA a The regular expression ab , made by combining a with b using concatena- tion is made by combining the two NFAs, hooking the head of a to the tail of b . The resulting machine has a tail labeled by a and a head into which the b edge °ows. 25 ","CHAPTER TWO. LEXICAL ANALYSIS a a M + constructed as M · M ∗ ϵ ∋ M ? constructed as M | ϵ M | N M N ∋ ∋ ∋ [ abc ] a ∋ b c M · N M N  abc  constructed as a · b · c M ∗ M ∋ ∋ FIGURE 2.6. Translation of regular expressions to NFAs. b a In general, any regular expression M will have some NFA with a tail and head: M We can deﬁne the translation of regular expressions to NFAs by induc- tion. Either an expression is primitive (a single symbol or ϵ ) or it is made from smaller expressions. Similarly, the NFA will be primitive or made from smaller NFAs. Figure 2.6 shows the rules for translating regular expressions to nonde- terministic automata. We illustrate the algorithm on some of the expressions in Figure 2.2 – for the tokens IF , ID , NUM , and error . Each expression is translated to an NFA, the “head” state of each NFA is marked ﬁnal with a dif- ferent token type, and the tails of all the expressions are joined to a new start node. The result – after some merging of equivalent NFA states – is shown in Figure 2.7 . 26 ","2.4. NONDETERMINISTIC FINITE AUTOMATA 1 any character 3 2 f IF 5 z a 4 . . . 0-9 a-z 6 . . . ID 10 9 0 9 . . . 9 0 11 7 12 . . . NUM 14 . . . error 15 i 8 13 FIGURE 2.7. Four regular expressions translated to an NFA. CONVERTING AN NFA TO A DFA As we saw in Section 2.3 , implementing deterministic ﬁnite automata (DFAs) as computer programs is easy. But implementing NFAs is a bit harder, since most computers don’t have good “guessing” hardware. We can avoid the need to guess by trying every possibility at once. Let us simulate the NFA of Figure 2.7 on the string in . We start in state 1. Now, instead of guessing which ϵ -transition to take, we just say that at this point the NFA might take any of them, so it is in one of the states { 1 , 4 , 9 , 14 } ; that is, we compute the ϵ -closure of { 1 } . Clearly, there are no other states reachable without eating the ﬁrst character of the input. Now, we make the transition on the character i . From state 1 we can reach 2, from 4 we reach 5, from 9 we go nowhere, and from 14 we reach 15. So we have the set { 2 , 5 , 15 } . But again we must compute ϵ -closure: from 5 there is an ϵ -transition to 8, and from 8 to 6. So the NFA must be in one of the states { 2 , 5 , 6 , 8 , 15 } . On the character n , we get from state 6 to 7, from 2 to nowhere, from 5 to nowhere, from 8 to nowhere, and from 15 to nowhere. So we have the set { 7 } ; its ϵ -closure is { 6 , 7 , 8 } . Now we are at the end of the string in ; is the NFA in a ﬁnal state? One of the states in our possible-states set is 8, which is ﬁnal. Thus, in is an ID token. We formally deﬁne ϵ -closure as follows. Let edge ( s , c ) be the set of all NFA states reachable by following a single edge with label c from state s . 27 ","CHAPTER TWO. LEXICAL ANALYSIS For a set of states S , closure ( S ) is the set of states that can be reached from a state in S without consuming any of the input, that is, by going only through ϵ edges. Mathematically, we can express the idea of going through ϵ edges by saying that closure ( S ) is smallest set T such that T = S ∪ ± ² s ∈ T edge ( s , ϵ ) ³ . We can calculate T by iteration: T ← S repeat T ′ ← T T ← T ′ ∪ ( ´ s ∈ T ′ edge ( s , ϵ )) until T = T ′ Why does this algorithm work? T can only grow in each iteration, so the ﬁnal T must include S . If T = T ′ after an iteration step, then T must also in- clude ´ s ∈ T ′ edge ( s , ϵ ) . Finally, the algorithm must terminate, because there are only a ﬁnite number of distinct states in the NFA. Now, when simulating an NFA as described above, suppose we are in a set d = { s i , s k , s l } of NFA states s i , s k , s l . By starting in d and eating the input symbol c , we reach a new set of NFA states; we’ll call this set DFAedge ( d , c ) : DFAedge ( d , c ) = closure ( ² s ∈ d edge ( s , c )) Using DFAedge , we can write the NFA simulation algorithm more formally. If the start state of the NFA is s 1 , and the input string is c 1 , . . . , c k , then the algorithm is: d ← closure ( { s 1 } ) for i ← 1 to k d ← DFAedge ( d , c i ) Manipulating sets of states is expensive – too costly to want to do on every character in the source program that is being lexically analyzed. But it is possible to do all the sets-of-states calculations in advance. We make a DFA from the NFA, such that each set of NFA states corresponds to one DFA state. Since the NFA has a ﬁnite number n of states, the DFA will also have a ﬁnite number (at most 2 n ) of states. DFA construction is easy once we have closure and DFAedge algorithms. The DFA start state d 1 is just closure ( s 1 ) , as in the NFA simulation algo- 28 ","2.4. NONDETERMINISTIC FINITE AUTOMATA a-h j-z IF ID NUM error i 1,4,9,14 2,5,6,8,15 5,6,8,15 10,11,13,15 6,7,8 11,12,13 15 0-9 f a-z 0-9 a-z 0-9 other 0-9 ID NUM a-z 0-9 0-9 ID 3,6,7,8 a-e , g-z , 0-9 FIGURE 2.8. NFA converted to DFA. rithm. Abstractly, there is an edge from d i to d j labeled with c if d j = DFAedge ( d i , c ) . We let ± be the alphabet. states [ 0 ] ← {}; states [ 1 ] ← closure ( { s 1 } ) p ← 1 ; j ← 0 while j ≤ p foreach c ∈ ± e ← DFAedge ( states [ j ] , c ) if e = states [ i ] for some i ≤ p then trans [ j , c ] ← i else p ← p + 1 states [ p ] ← e trans [ j , c ] ← p j ← j + 1 The algorithm does not visit unreachable states of the DFA. This is ex- tremely important, because in principle the DFA has 2 n states, but in practice we usually ﬁnd that only about n of them are reachable from the start state. It is important to avoid an exponential blowup in the size of the DFA inter- preter’s transition tables, which will form part of the working compiler. A state d is ﬁnal in the DFA if any NFA-state in states [ d ] is ﬁnal in the NFA. Labeling a state ﬁnal is not enough; we must also say what token is recognized; and perhaps several members of states [ d ] are ﬁnal in the NFA. In this case we label d with the token-type that occurred ﬁrst in the list of 29 ","CHAPTER TWO. LEXICAL ANALYSIS regular expressions that constitute the lexical speciﬁcation. This is how rule priority is implemented. After the DFA is constructed, the “states” array may be discarded, and the “trans” array is used for lexical analysis. Applying the DFA construction algorithm to the NFA of Figure 2.7 gives the automaton in Figure 2.8 . This automaton is suboptimal. That is, it is not the smallest one that recog- nizes the same language. In general, we say that two states s 1 and s 2 are equiv- alent when the machine starting in s 1 accepts a string σ if and only if starting in s 2 it accepts σ . This is certainly true of the states labeled 5,6,8,15 and 6,7,8 in Figure 2.8 ; and of the states labeled 10,11,13,15 and 11,12,13 . In an automaton with two equivalent states s 1 and s 2 , we can make all of s 2 ’s incoming edges point to s 1 instead and delete s 2 . How can we ﬁnd equivalent states? Certainly, s 1 and s 2 are equivalent if they are both ﬁnal or both non-ﬁnal and for any symbol c , trans [ s 1 , c ] = trans [ s 2 , c ] ; 10,11,13,15 and 11,12,13 satisfy this criterion. But this con- dition is not sufﬁciently general; consider the automaton 1 3 2 a a 5 4 a a a b Here, states 2 and 4 are equivalent, but trans [ 2 , a ] ̸ = trans [ 4 , a ] . After constructing a DFA it is useful to apply an algorithm to minimize it by ﬁnding equivalent states; see Exercise 2.6 . 2.5 Lex: A LEXICAL ANALYZER GENERATOR DFA construction is a mechanical task easily performed by computer, so it makes sense to have an automatic lexical analyzer generator to translate reg- ular expressions into a DFA. Lex is a lexical analyzer generator that produces a C program from a lexi- cal speciﬁcation . For each token type in the programming language to be lex- ically analyzed, the speciﬁcation contains a regular expression and an action . 30 ","2.5. LEX: A LEXICAL ANALYZER GENERATOR %{ /* C Declarations: */ #include tokens.h /* deﬁnitions of IF, ID, NUM, ... */ #include errormsg.h union {int ival; string sval; double fval;} yylval; int charPos=1; #define ADJ (EM_tokPos=charPos, charPos+=yyleng) %} /* Lex Deﬁnitions: */ digits [0-9]+ %% /* Regular Expressions and Actions: */ if {ADJ; return IF;} [a-z][a-z0-9]* {ADJ; yylval.sval=String(yytext); return ID;} {digits} {ADJ; yylval.ival=atoi(yytext); return NUM;} ({digits}.[0-9]*)|([0-9]*.{digits}) {ADJ; yylval.fval=atof(yytext); return REAL;} (--[a-z]*n)|( |n|t)+ {ADJ;} . {ADJ; EM_error(illegal character);} PROGRAM 2.9. Lex speciﬁcation of the tokens from Figure 2.2 . The action communicates the token type (perhaps along with other informa- tion) to the next phase of the compiler. The output of Lex is a program in C – a lexical analyzer that interprets a DFA using the algorithm described in Section 2.3 and executes the action fragments on each match. The action fragments are just C statements that return token values. The tokens described in Figure 2.2 are speciﬁed in Lex as shown in Pro- gram 2.9. The ﬁrst part of the speciﬁcation, between the %{ · · · %} braces, contains include s and declarations that may be used by the C code in the remainder of the ﬁle. The second part of the speciﬁcation contains regular-expression abbrevi- ations and state declarations. For example, the declaration digits [0-9]+ in this section allows the name {digits} to stand for a nonempty sequence of digits within regular expressions. The third part contains regular expressions and actions. The actions are fragments of ordinary C code. Each action must return a value of type int , denoting which kind of token has been found. 31 ","CHAPTER TWO. LEXICAL ANALYSIS In the action fragments, several special variables are available. The string matched by the regular expression is yytext . The length of the matched string is yyleng . In this particular example, we keep track of the position of each token, measured in characters since the beginning of the ﬁle, in the variable char- Pos . The EM_tokPos variable of the error message module errormsg.h is continually told this position by calls to the macro ADJ . The parser will be able to use this information in printing informative syntax error messages. The include ﬁle tokens.h in this example deﬁnes integer constants IF , ID , NUM , and so on; these values are returned by the action fragments to tell what token-type is matched. Some tokens have semantic values associated with them. For example, ID ’s semantic value is the character string constituting the identiﬁer; NUM ’s se- mantic value is an integer; and IF has no semantic value (any IF is indistin- guishable from any other). The values are communicated to the parser through the global variable yylval , which is a union of the different types of seman- tic values. The token-type returned by the lexer tells the parser which variant of the union is valid. START STATES Regular expressions are static and declarative ; automata are dynamic and imperative . That is, you can see the components and structure of a regular expression without having to simulate an algorithm, but to understand an automaton it is often necessary to “execute” it in your mind. Thus, regular expressions are usually more convenient to specify the lexical structure of programming-language tokens. But sometimes the step-by-step, state-transition model of automata is ap- propriate. Lex has a mechanism to mix states with regular expressions. One can declare a set of start states ; each regular expression can be preﬁxed by the set of start states in which it is valid. The action fragments can explic- itly change the start state. In effect, we have a ﬁnite automaton whose edges are labeled, not by single symbols, but by regular expressions. This example shows a language with simple identiﬁers, if tokens, and comments delimited by (* and *) brackets: if (* *) [a-z]+ . INITIAL COMMENT 32 ","PROGRAMMING EXERCISE Though it is possible to write a single regular expression that matches an en- tire comment, as comments get more complicated it becomes more difﬁcult, or even impossible if nested comments are allowed. The Lex speciﬁcation corresponding to this machine is . . . the usual preamble ... %Start INITIAL COMMENT %% &lt;INITIAL&gt;if {ADJ; return IF;} &lt;INITIAL&gt;[a-z]+ {ADJ; yylval.sval=String(yytext); return ID;} &lt;INITIAL&gt;(* {ADJ; BEGIN COMMENT;} &lt;INITIAL&gt;. {ADJ; EM_error(illegal character);} &lt;COMMENT&gt;*) {ADJ; BEGIN INITIAL;} &lt;COMMENT&gt;. {ADJ;} . {BEGIN INITIAL; yyless(1);} where INITIAL is the “outside of any comment” state. The last rule is a hack to get Lex into this state. Any regular expression not preﬁxed by a &lt;STATE&gt; operates in all states; this feature is rarely useful. This example can be easily augmented to handle nested comments, via a global variable that is incremented and decremented in the semantic actions. P R O G R A M LEXICAL ANALYSIS Use Lex to implement a lexical analyzer for the Tiger language. Appendix A describes, among other things, the lexical tokens of Tiger. This chapter has left out some of the speciﬁcs of how the lexical analyzer should be initialized and how it should communicate with the rest of the com- piler. You can learn this from the Lex manual, but the “skeleton” ﬁles in the $TIGER/chap2 directory will also help get you started. Along with the tiger.lex ﬁle you should turn in documentation for the following points: • how you handle comments; • how you handle strings; • error handling; • end-of-ﬁle handling; • other interesting features of your lexer. 33 "]